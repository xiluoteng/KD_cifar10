{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "053fc747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomGrayscale, ToTensor, Normalize\n",
    "from torchsummary import summary\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss,  Sequential, CrossEntropyLoss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90c08c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ac08752310>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainBS = 64\n",
    "TestBS = 32\n",
    "Learning_Rate = 1e-4\n",
    "KD_temp = 10\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "Random_Seed = np.random.uniform()\n",
    "torch.manual_seed(Random_Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4865a56b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "Train_Data = DataLoader(dataset = CIFAR10(train = True,\n",
    "                                          root = './data/',\n",
    "                                          download = True,\n",
    "                                          transform = Compose([RandomHorizontalFlip(),\n",
    "                                                               RandomGrayscale(),\n",
    "                                                               ToTensor(),\n",
    "                                                               Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])),\n",
    "                       batch_size = TrainBS,\n",
    "                       shuffle = True)\n",
    "Test_Data = DataLoader(dataset = CIFAR10(train = False,\n",
    "                                         root = './data/',\n",
    "                                         download = True,\n",
    "                                         transform = Compose([ToTensor(),\n",
    "                                                              Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])),\n",
    "                       batch_size = TestBS,\n",
    "                       shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d7bade4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d, AdaptiveAvgPool2d, Linear, Softmax\n",
    "from torchvision.transforms import Resize, InterpolationMode\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Basic_Block(Module):\n",
    "    def __init__(self, Proc_Channel, DownSample):\n",
    "        super(Basic_Block, self).__init__()\n",
    "        Proc_Channel= int(Proc_Channel)\n",
    "        stride = 1\n",
    "        in_channels = int(Proc_Channel)\n",
    "        self.shortcut = Sequential()\n",
    "        if DownSample == 1:\n",
    "            if Proc_Channel != 64:\n",
    "                in_channels = int(Proc_Channel/2)\n",
    "                stride = 2\n",
    "            self.shortcut = Sequential(Conv2d(in_channels = int(in_channels), \n",
    "                                   out_channels = int(Proc_Channel), \n",
    "                                   kernel_size = 3, \n",
    "                                   stride = int(stride), \n",
    "                                   padding = 1),\n",
    "                               BatchNorm2d(Proc_Channel)) \n",
    "        self.ConvLayer1 = Sequential(Conv2d(in_channels, Proc_Channel, 3, stride, 1),\n",
    "                            BatchNorm2d(Proc_Channel))\n",
    "        self.ConvLayer2 = Sequential(Conv2d(Proc_Channel, Proc_Channel, 3, 1, 1),\n",
    "                            BatchNorm2d(Proc_Channel)) \n",
    "    def forward(self, x):\n",
    "        Residual = self.shortcut(x)\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x + Residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Bottle_neck(nn.Module):\n",
    "    def __init__(self, Proc_Channel, DownSample):\n",
    "        super(Bottle_neck,self).__init__()\n",
    "        Proc_Channel = int(Proc_Channel)\n",
    "        stride = 1\n",
    "        in_channels = int(Proc_Channel * 4)\n",
    "        if DownSample == 1:\n",
    "            if Proc_Channel == 64:\n",
    "                in_channels = Proc_Channel\n",
    "            else:\n",
    "                stride = 2\n",
    "                in_channels = int(Proc_Channel * 2)\n",
    "        self.ConvLayer1 = Sequential(Conv2d(in_channels, Proc_Channel, 1, stride, 0),\n",
    "                            BatchNorm2d(Proc_Channel), \n",
    "                            ReLU())\n",
    "        self.ConvLayer2 = Sequential(Conv2d(Proc_Channel, Proc_Channel, 3, stride, 1),\n",
    "                            BatchNorm2d(Proc_Channel), \n",
    "                            ReLU())\n",
    "        self.ConvLayer3 = Sequential(Conv2d(Proc_Channel, Proc_Channel * 4, 1, stride, 0),\n",
    "                            BatchNorm2d(Proc_Channel * 4), \n",
    "                            ReLU())\n",
    "        self.shortcut = Sequential(Conv2d(in_channels, Proc_Channel * 4, 3, stride, 1),\n",
    "                          BatchNorm2d(Proc_Channel * 4))\n",
    "    def forward(self,x):\n",
    "        Residual = self.shortcut(x)\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = self.ConvLayer3(x)\n",
    "        x = x + Residual\n",
    "        return x\n",
    "class Resnet(Module):\n",
    "    arch = {18 : [Basic_Block, [2, 2, 2, 2], 512], \n",
    "             34 : [Basic_Block, [3, 4, 6, 3], 512], \n",
    "            50 : [Bottle_neck, [3, 4, 6, 3], 2048], \n",
    "            101 : [Bottle_neck, [3, 4, 23, 3], 2048], \n",
    "            152 : [Bottle_neck, [3, 8, 36, 3], 2048]}\n",
    "    def __init__(self,typ):\n",
    "        super(Resnet,self).__init__()\n",
    "        [block, layer_arch, final_channel] = Resnet.arch[typ]\n",
    "        self.Resize = Resize((224,224), interpolation = InterpolationMode.BILINEAR)\n",
    "        self.final_channel = final_channel\n",
    "        self.stem = Sequential(Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        BatchNorm2d(64),\n",
    "                        ReLU(),\n",
    "                        MaxPool2d(kernel_size = 3, stride = 2, padding = 1))\n",
    "        self.stage1 = self.make_layer(block, 64, layer_arch[0], stride = 1)\n",
    "        self.stage2 = self.make_layer(block, 128, layer_arch[1], stride = 2)\n",
    "        self.stage3 = self.make_layer(block, 256, layer_arch[2], stride = 2)\n",
    "        self.stage4 = self.make_layer(block, 512, layer_arch[3], stride = 2)\n",
    "        self.Aver_pool = AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = Linear(self.final_channel, 10)\n",
    "        self.softmax = Softmax(dim = 1)\n",
    "    def make_layer(self, block, Proc_Channel, layer_arch, stride):\n",
    "        layer = []\n",
    "        for i in range(layer_arch):\n",
    "            if i ==0 :\n",
    "                layer.append(block(Proc_Channel,True))\n",
    "            else:\n",
    "                layer.append(block(Proc_Channel,False))\n",
    "        return Sequential(*layer)\n",
    "    def forward(self,x):\n",
    "        x = self.Resize(x)\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.Aver_pool(x)\n",
    "        x = x.view(-1, self.final_channel)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae35c1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTeacher_Network = Resnet(34)\\nTeacher_Network = Teacher_Network.to('cuda')\\noptimizer = Adam(params = Teacher_Network.parameters(), lr = Learning_Rate)  \\nLoss_Function = CrossEntropyLoss()\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Teacher_Network = Resnet(34)\n",
    "Teacher_Network = Teacher_Network.to('cuda')\n",
    "optimizer = Adam(params = Teacher_Network.parameters(), lr = Learning_Rate)  \n",
    "Loss_Function = CrossEntropyLoss()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21bf53ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nepochs =  100\\nfor epoch in range(1,epochs + 1):\\n    Teacher_Network.train()\\n    correct = 0\\n    avg_loss = 0\\n    for batch_idx, (data, target) in enumerate(tqdm(Train_Data)):\\n        data = data.to('cuda')\\n        target = target.to('cuda')\\n        label = np.zeros((len(target),  10))\\n        for idx,i in enumerate(target):\\n            label[idx][i] = 1\\n        label = torch.tensor(label, dtype = torch.float32)\\n        label = label.to('cuda')\\n        optimizer.zero_grad()\\n        output = Teacher_Network(data)\\n        output = output.view(-1,10)\\n        loss = Loss_Function (output, label)\\n        loss.backward()\\n        optimizer.step()\\n        avg_loss += loss\\n        correct += target.eq(output.data.max(1).indices).sum()\\n    print('Train_Epoch:{}\\t Loss:{:.6f}\\t Acc:{:.1f}'.format(\\n        epoch, avg_loss / len(Train_Data), 100.*correct / len(Train_Data.dataset)))\\n    avg_loss /= len(Train_Data) \\n    acc = correct / len(Train_Data.dataset)\\n    with open('result.txt', 'a') as f:\\n        f.write('{'Epoch': ')\\n        f.write(str(epoch))\\n        f.write(', 'Avg_Loss': ')\\n        f.write(str(avg_loss.item()))\\n        f.write(', 'Acc': ')\\n        f.write(str(acc.item()))\\n        f.write('}\\n')\\n    if epoch%10 ==0:\\n        torch.save(Teacher_Network.state_dict(), f'Res34_{epoch}.pth')\\ntorch.save(Teacher_Network.state_dict(), f'Res34_Last.pth')\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "epochs =  100\n",
    "for epoch in range(1,epochs + 1):\n",
    "    Teacher_Network.train()\n",
    "    correct = 0\n",
    "    avg_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(Train_Data)):\n",
    "        data = data.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "        label = np.zeros((len(target),  10))\n",
    "        for idx,i in enumerate(target):\n",
    "            label[idx][i] = 1\n",
    "        label = torch.tensor(label, dtype = torch.float32)\n",
    "        label = label.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        output = Teacher_Network(data)\n",
    "        output = output.view(-1,10)\n",
    "        loss = Loss_Function (output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss\n",
    "        correct += target.eq(output.data.max(1).indices).sum()\n",
    "    print('Train_Epoch:{}\\t Loss:{:.6f}\\t Acc:{:.1f}'.format(\n",
    "        epoch, avg_loss / len(Train_Data), 100.*correct / len(Train_Data.dataset)))\n",
    "    avg_loss /= len(Train_Data) \n",
    "    acc = correct / len(Train_Data.dataset)\n",
    "    with open('result.txt', 'a') as f:\n",
    "        f.write('{\\'Epoch\\': ')\n",
    "        f.write(str(epoch))\n",
    "        f.write(', \\'Avg_Loss\\': ')\n",
    "        f.write(str(avg_loss.item()))\n",
    "        f.write(', \\'Acc\\': ')\n",
    "        f.write(str(acc.item()))\n",
    "        f.write('}\\n')\n",
    "    if epoch%10 ==0:\n",
    "        torch.save(Teacher_Network.state_dict(), f'Res34_{epoch}.pth')\n",
    "torch.save(Teacher_Network.state_dict(), f'Res34_Last.pth')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aef6ea9e-b818-4e9b-ba0c-ed40dc27467b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nStudent_Network = Resnet(18)\\nStudent_Network = Student_Network.to('cuda')\\noptimizer = Adam(params = Student_Network.parameters(), lr = Learning_Rate)  \\nLoss_Function = CrossEntropyLoss()\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Student_Network = Resnet(18)\n",
    "Student_Network = Student_Network.to('cuda')\n",
    "optimizer = Adam(params = Student_Network.parameters(), lr = Learning_Rate)  \n",
    "Loss_Function = CrossEntropyLoss()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28e2827a-a632-45b2-b05f-f7e4cc57d15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nepochs = 20\\nfor epoch in range(1, epochs + 1):\\n    Student_Network.train()\\n    avg_loss = 0\\n    acc = 0\\n    for batch_idx, (data, target) in enumerate(tqdm(Train_Data)):\\n        #data = data.to('cuda')\\n        #target = target.to('cuda')\\n        label = np.zeros((len(target),10))\\n        for label_idx, i in enumerate(target):\\n            label[label_idx][i] = 1\\n        label = torch.tensor(label, dtype = torch.float32)\\n        #label = label.to('cuda')\\n        output = Student_Network(data)\\n        loss = Loss_Function(output, label)\\n        loss.backward()\\n        optimizer.step()\\n        avg_loss += loss\\n        correct = target.eq(data.max(1).indices).sum()\\n    print('Train_Epoch: {}\\t Loss: {:.6f}\\t Acc: {:.1f}'.format\\n          (epoch, avg_loss /= len(Train_Date), 100.*correct/len(Train_Data.dataset)))\\n    acc = correct / len(Train_Data.dataset)\\n    avg_loss /= len(Train_Data)\\n    with open('ResNet18_result.txt', 'a') as f:\\n        f.write('{'Epoch': ')\\n        f.write(str(epoch))\\n        f.write(', 'Avg_Loss': ')\\n        f.write(str(avg_loss.item()))\\n        f.write(', 'Acc': ')\\n        f.write(str(acc.item()))\\n        f.write('}\\n')\\n    Student_Network.eval()\\n    correct = 0\\n    for batch_idx, (data, target) in enumerate(tqdm(Test_Data)):\\n        #data = data.to('cuda')\\n        #target = target.to('cuda')\\n        with torch.no_grad():\\n            output = Student_Network(data)\\n            correct += target.eq(output.max(1).indices).sum()\\n    print( format( correct / len( Test_Data.dataset ) * 100, '.2f') )\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs + 1):\n",
    "    Student_Network.train()\n",
    "    avg_loss = 0\n",
    "    acc = 0\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(Train_Data)):\n",
    "        #data = data.to('cuda')\n",
    "        #target = target.to('cuda')\n",
    "        label = np.zeros((len(target),10))\n",
    "        for label_idx, i in enumerate(target):\n",
    "            label[label_idx][i] = 1\n",
    "        label = torch.tensor(label, dtype = torch.float32)\n",
    "        #label = label.to('cuda')\n",
    "        output = Student_Network(data)\n",
    "        loss = Loss_Function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss\n",
    "        correct = target.eq(data.max(1).indices).sum()\n",
    "    print('Train_Epoch: {}\\t Loss: {:.6f}\\t Acc: {:.1f}'.format\n",
    "          (epoch, avg_loss /= len(Train_Date), 100.*correct/len(Train_Data.dataset)))\n",
    "    acc = correct / len(Train_Data.dataset)\n",
    "    avg_loss /= len(Train_Data)\n",
    "    with open('ResNet18_result.txt', 'a') as f:\n",
    "        f.write('{\\'Epoch\\': ')\n",
    "        f.write(str(epoch))\n",
    "        f.write(', \\'Avg_Loss\\': ')\n",
    "        f.write(str(avg_loss.item()))\n",
    "        f.write(', \\'Acc\\': ')\n",
    "        f.write(str(acc.item()))\n",
    "        f.write('}\\n')\n",
    "    Student_Network.eval()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(Test_Data)):\n",
    "        #data = data.to('cuda')\n",
    "        #target = target.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            output = Student_Network(data)\n",
    "            correct += target.eq(output.max(1).indices).sum()\n",
    "    print( format( correct / len( Test_Data.dataset ) * 100, '.2f') )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfe40199",
   "metadata": {},
   "outputs": [],
   "source": [
    "Teacher_Network = Resnet(34)\n",
    "Student_Network = Resnet(18)\n",
    "Teacher_Network.load_state_dict(torch.load('Res34_Last.pth'))\n",
    "Teacher_Network = Teacher_Network.to('cuda')\n",
    "Student_Network = Student_Network.to('cuda')\n",
    "optimizer = Adam(params = Student_Network.parameters(), lr = Learning_Rate)\n",
    "Loss_Function = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4280b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                                                               | 2/782 [00:36<3:57:06, 18.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m Normal_Loss \u001b[38;5;241m=\u001b[39m Loss_Function(Student_Pred, Pred)\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m Distill_Loss \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m+\u001b[39m Normal_Loss \u001b[38;5;241m*\u001b[39m beta\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m kd_avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Distill_Loss\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "Teacher_Network.eval()\n",
    "for epochs in range(1, epochs + 1):\n",
    "    Student_Network.train()\n",
    "    correct = 0\n",
    "    kd_avg_loss = 0\n",
    "    gt_avg_loss = 0\n",
    "    for batch_idx, (data,target) in enumerate(tqdm(Train_Data)):\n",
    "        data = data.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "        Pred = np.zeros((len(target), 10))\n",
    "        for idx, i in enumerate(target):\n",
    "            Pred[idx][i] = 1\n",
    "        Pred = torch.tensor(Pred, dtype = torch.float32)\n",
    "        Pred = label.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            Teacher_Pred = Teacher_Network(data)\n",
    "        Student_Pred = Student_Network(data)\n",
    "        Distill_Loss = Loss_Function(Student_Pred / KD_temp, Teacher_Pred / KD_temp)\n",
    "        Normal_Loss = Loss_Function(Student_Pred, Pred)\n",
    "        loss = Distill_Loss * alpha + Normal_Loss * beta\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        kd_avg_loss += Distill_Loss\n",
    "        gt_avg_loss += Normal_Loss\n",
    "        teacher_correct = target.eq(Teacher_Pred.max(1).indices).sum()\n",
    "        student_correct = target.eq(Student_Pred.max(1).indices).sum()\n",
    "    print('Epoch:{}\\t KDLoss:{:.6f}\\t GTLoss:{:.6f}\\t TAcc:{:.1f}\\t Acc:{:.1f}\\t'.format(\n",
    "         epoch, kd_avg_loss / len(Train_Data), gt_avg_loss / len(Train_Data), \n",
    "          teacher_correct / len(Train_Data.dataset), student_correct / len(Train_Data.dataset)))\n",
    "    Acc = student_correct / len(Train_Data.dataset)\n",
    "    Tacc = teacher_correct / len(Train_Data.dataset)\n",
    "    gt_avg_loss /= len(Train_Data)\n",
    "    kd_avg_loss /= len(Train_Data)\n",
    "    with open('Resnet18KD.txt', 'a') as f:\n",
    "        f.write('{\\\"Epoch\\\": ')\n",
    "        f.write(str(epoch))\n",
    "        f.write(', \\\"KDLoss\\\": ')\n",
    "        f.write(str(kd_avg_loss.item()))\n",
    "        f.write(', \\\"GTLoss\\\": ')\n",
    "        f.write(str(gt_avg_loss.item()))\n",
    "        f.write(', \\\"TAcc\\\": ')\n",
    "        f.write(str(Tacc.item()))\n",
    "        f.write(', \\\"Acc\\\": ')\n",
    "        f.write(str(Acc.item()))\n",
    "        f.write('}\\n')\n",
    "    Student_Network.eval\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(Test_Data)):\n",
    "        data = data.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "        with torch.no_grad:\n",
    "            output = Student_Network(data)\n",
    "            correct += target.eq(output.max(1).indice).sum()\n",
    "    print( format( correct / len( Test_Data.dataset ) * 100, '.2f') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116c647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
