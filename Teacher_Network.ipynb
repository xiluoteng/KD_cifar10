{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053fc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomGrayscale, ToTensor, Normalize\n",
    "from torchsummary import summary\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss,  Sequential, CrossEntropyLoss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c08c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fdffb44b70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainBS = 64\n",
    "TestBS = 64\n",
    "Learning_Rate = 1e-4\n",
    "Random_Seed = np.random.uniform()\n",
    "torch.manual_seed(Random_Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4865a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "Train_Data = DataLoader(dataset = CIFAR10(train = True,\n",
    "                                          root = '/data/',\n",
    "                                          download = True,\n",
    "                                          transform = Compose([RandomHorizontalFlip(),\n",
    "                                                               RandomGrayscale(),\n",
    "                                                               ToTensor(),\n",
    "                                                               Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])),\n",
    "                       batch_size = TrainBS,\n",
    "                       shuffle = True)\n",
    "Test_Data = DataLoader(dataset = CIFAR10(train = False,\n",
    "                                         root = '/data/',\n",
    "                                         download = True,\n",
    "                                         transform = Compose([ToTensor(),\n",
    "                                                              Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])),\n",
    "                       batch_size = TestBS,\n",
    "                       shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7bade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d, AdaptiveAvgPool2d, Linear, Softmax\n",
    "from torchvision.transforms import Resize, InterpolationMode\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Basic_Block(Module):\n",
    "    def __init__(self, Proc_Channel, DownSample):\n",
    "        super(Basic_Block, self).__init__()\n",
    "        Proc_Channel= int(Proc_Channel)\n",
    "        stride = 1\n",
    "        in_channels = int(Proc_Channel)\n",
    "        self.shortcut = Sequential()\n",
    "        if DownSample == 1:\n",
    "            if Proc_Channel != 64:\n",
    "                in_channels = int(Proc_Channel/2)\n",
    "                stride = 2\n",
    "            self.shortcut = Sequential(Conv2d(in_channels = int(in_channels), \n",
    "                                   out_channels = int(Proc_Channel), \n",
    "                                   kernel_size = 3, \n",
    "                                   stride = int(stride), \n",
    "                                   padding = 1),\n",
    "                               BatchNorm2d(Proc_Channel)) \n",
    "        self.ConvLayer1 = Sequential(Conv2d(in_channels, Proc_Channel, 3, stride, 1),\n",
    "                            BatchNorm2d(Proc_Channel))\n",
    "        self.ConvLayer2 = Sequential(Conv2d(Proc_Channel, Proc_Channel, 3, 1, 1),\n",
    "                            BatchNorm2d(Proc_Channel)) \n",
    "    def forward(self, x):\n",
    "        Residual = self.shortcut(x)\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x + Residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Bottle_neck(nn.Module):\n",
    "    def __init__(self, Proc_Channel, DownSample):\n",
    "        super(Bottle_neck,self).__init__()\n",
    "        Proc_Channel = int(Proc_Channel)\n",
    "        stride = 1\n",
    "        in_channels = int(Proc_Channel * 4)\n",
    "        if DownSample == 1:\n",
    "            if Proc_Channel == 64:\n",
    "                in_channels = Proc_Channel\n",
    "            else:\n",
    "                stride = 2\n",
    "                in_channels = int(Proc_Channel * 2)\n",
    "        self.ConvLayer1 = Sequential(Conv2d(in_channels, Proc_Channel, 1, stride, 0),\n",
    "                            BatchNorm2d(Proc_Channel), \n",
    "                            ReLU())\n",
    "        self.ConvLayer2 = Sequential(Conv2d(Proc_Channel, Proc_Channel, 3, stride, 1),\n",
    "                            BatchNorm2d(Proc_Channel), \n",
    "                            ReLU())\n",
    "        self.ConvLayer3 = Sequential(Conv2d(Proc_Channel, Proc_Channel * 4, 1, stride, 0),\n",
    "                            BatchNorm2d(Proc_Channel * 4), \n",
    "                            ReLU())\n",
    "        self.shortcut = Sequential(Conv2d(in_channels, Proc_Channel * 4, 3, stride, 1),\n",
    "                          BatchNorm2d(Proc_Channel * 4))\n",
    "    def forward(self,x):\n",
    "        Residual = self.shortcut(x)\n",
    "        x = self.ConvLayer1(x)\n",
    "        x = self.ConvLayer2(x)\n",
    "        x = self.ConvLayer3(x)\n",
    "        x = x + Residual\n",
    "        return x\n",
    "class Resnet(Module):\n",
    "    arch = {18 : [Basic_Block, [2, 2, 2, 2], 512], \n",
    "             34 : [Basic_Block, [3, 4, 6, 3], 512], \n",
    "            50 : [Bottle_neck, [3, 4, 6, 3], 2048], \n",
    "            101 : [Bottle_neck, [3, 4, 23, 3], 2048], \n",
    "            152 : [Bottle_neck, [3, 8, 36, 3], 2048]}\n",
    "    def __init__(self,typ):\n",
    "        super(Resnet,self).__init__()\n",
    "        [block, layer_arch, final_channel] = Resnet.arch[typ]\n",
    "        self.Resize = Resize((224,224), interpolation = InterpolationMode.BILINEAR)\n",
    "        self.final_channel = final_channel\n",
    "        self.stem = Sequential(Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        BatchNorm2d(64),\n",
    "                        ReLU(),\n",
    "                        MaxPool2d(kernel_size = 3, stride = 2, padding = 1))\n",
    "        self.stage1 = self.make_layer(block, 64, layer_arch[0], stride = 1)\n",
    "        self.stage2 = self.make_layer(block, 128, layer_arch[1], stride = 2)\n",
    "        self.stage3 = self.make_layer(block, 256, layer_arch[2], stride = 2)\n",
    "        self.stage4 = self.make_layer(block, 512, layer_arch[3], stride = 2)\n",
    "        self.Aver_pool = AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = Linear(self.final_channel, 10)\n",
    "        self.softmax = Softmax(dim = 1)\n",
    "    def make_layer(self, block, Proc_Channel, layer_arch, stride):\n",
    "        layer = []\n",
    "        for i in range(layer_arch):\n",
    "            if i ==0 :\n",
    "                layer.append(block(Proc_Channel,True))\n",
    "            else:\n",
    "                layer.append(block(Proc_Channel,False))\n",
    "        return Sequential(*layer)\n",
    "    def forward(self,x):\n",
    "        x = self.Resize(x)\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.Aver_pool(x)\n",
    "        x = x.view(-1, self.final_channel)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "def Resnet18():\n",
    "    return Resnet(Basic_Block, [2,2,2,2], 512)\n",
    "def Resnet34():\n",
    "    return Resnet(Basic_Block, [3,4,6,3], 512)\n",
    "def Resnet50():\n",
    "    return Resnet(Bottle_neck, [3,4,6,3] , 2048)\n",
    "def Resnet101():\n",
    "    return Resnet(Bottle_neck, [3,4,23,3] , 2048)\n",
    "def Resnet152():\n",
    "    return Resnet(Bottle_neck, [3,8,36,3] , 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aaf1677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Resize-1          [-1, 3, 224, 224]               0\n",
      "            Conv2d-2         [-1, 64, 112, 112]           9,472\n",
      "       BatchNorm2d-3         [-1, 64, 112, 112]             128\n",
      "              ReLU-4         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-5           [-1, 64, 56, 56]               0\n",
      "            Conv2d-6           [-1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "           Conv2d-10           [-1, 64, 56, 56]          36,928\n",
      "      BatchNorm2d-11           [-1, 64, 56, 56]             128\n",
      "      Basic_Block-12           [-1, 64, 56, 56]               0\n",
      "           Conv2d-13           [-1, 64, 56, 56]          36,928\n",
      "      BatchNorm2d-14           [-1, 64, 56, 56]             128\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,928\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "      Basic_Block-17           [-1, 64, 56, 56]               0\n",
      "           Conv2d-18           [-1, 64, 56, 56]          36,928\n",
      "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,928\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "      Basic_Block-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 128, 28, 28]          73,856\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "           Conv2d-25          [-1, 128, 28, 28]          73,856\n",
      "      BatchNorm2d-26          [-1, 128, 28, 28]             256\n",
      "           Conv2d-27          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-28          [-1, 128, 28, 28]             256\n",
      "      Basic_Block-29          [-1, 128, 28, 28]               0\n",
      "           Conv2d-30          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      "           Conv2d-32          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "      Basic_Block-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "           Conv2d-37          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
      "      Basic_Block-39          [-1, 128, 28, 28]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "      Basic_Block-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 256, 14, 14]         295,168\n",
      "      BatchNorm2d-46          [-1, 256, 14, 14]             512\n",
      "           Conv2d-47          [-1, 256, 14, 14]         295,168\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "           Conv2d-49          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-50          [-1, 256, 14, 14]             512\n",
      "      Basic_Block-51          [-1, 256, 14, 14]               0\n",
      "           Conv2d-52          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-53          [-1, 256, 14, 14]             512\n",
      "           Conv2d-54          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-55          [-1, 256, 14, 14]             512\n",
      "      Basic_Block-56          [-1, 256, 14, 14]               0\n",
      "           Conv2d-57          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-58          [-1, 256, 14, 14]             512\n",
      "           Conv2d-59          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
      "      Basic_Block-61          [-1, 256, 14, 14]               0\n",
      "           Conv2d-62          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-63          [-1, 256, 14, 14]             512\n",
      "           Conv2d-64          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-65          [-1, 256, 14, 14]             512\n",
      "      Basic_Block-66          [-1, 256, 14, 14]               0\n",
      "           Conv2d-67          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-68          [-1, 256, 14, 14]             512\n",
      "           Conv2d-69          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-70          [-1, 256, 14, 14]             512\n",
      "      Basic_Block-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "           Conv2d-74          [-1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "      Basic_Block-76          [-1, 256, 14, 14]               0\n",
      "           Conv2d-77            [-1, 512, 7, 7]       1,180,160\n",
      "      BatchNorm2d-78            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-79            [-1, 512, 7, 7]       1,180,160\n",
      "      BatchNorm2d-80            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-81            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-82            [-1, 512, 7, 7]           1,024\n",
      "      Basic_Block-83            [-1, 512, 7, 7]               0\n",
      "           Conv2d-84            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-85            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-86            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-87            [-1, 512, 7, 7]           1,024\n",
      "      Basic_Block-88            [-1, 512, 7, 7]               0\n",
      "           Conv2d-89            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-90            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-91            [-1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-92            [-1, 512, 7, 7]           1,024\n",
      "      Basic_Block-93            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-94            [-1, 512, 1, 1]               0\n",
      "           Linear-95                   [-1, 10]           5,130\n",
      "          Softmax-96                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 22,711,626\n",
      "Trainable params: 22,711,626\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 79.44\n",
      "Params size (MB): 86.64\n",
      "Estimated Total Size (MB): 166.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Network = Resnet(34)\n",
    "summary(Network, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae35c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Teacher_Network = Resnet(34)\n",
    "optimizer = Adam(params = Teacher_Network.parameters(), lr = Learning_Rate)  \n",
    "Loss_Function = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf53ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:10, 23.26s/it]"
     ]
    }
   ],
   "source": [
    "epochs =  1\n",
    "for epoch in range(1,epochs + 1):\n",
    "    Teacher_Network.train()\n",
    "    correct = 0\n",
    "    avg_loss = 0\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(Train_Data)):\n",
    "        label = np.zeros((len(target),  10))\n",
    "        for idx,i in enumerate(target):\n",
    "            label[idx][i] = 1\n",
    "        label = torch.tensor(label, dtype = torch.float32) \n",
    "        optimizer.zero_grad()\n",
    "        output = Teacher_Network(data)\n",
    "        output = output.view(-1,10)\n",
    "        loss = Loss_Function (output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss\n",
    "        correct += target.eq(output.data.max(1).indices).sum()\n",
    "    print('Train_Epoch:{}\\t Loss:{:.6f}\\t Acc:{:.1f}'.format(\n",
    "        epoch, avg_loss/len(Train_Data), 100.*correct/len(Train_Data.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0caacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Network.state_dict(), 'KD-resnet.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
