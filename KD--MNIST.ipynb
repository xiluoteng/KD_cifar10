{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "479827aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.nn import ReLU, Linear, Dropout, CrossEntropyLoss, NLLLoss\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchsummary import summary\n",
    "from torch import randn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee0f3fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e04ee4ad0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainBS = 128\n",
    "TestBS = 64\n",
    "Random_Seed = np.random.uniform()\n",
    "torch.manual_seed(Random_Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d5305eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data = DataLoader(dataset = MNIST(train = True,\n",
    "                                        root = '/data/',\n",
    "                                        download = True,\n",
    "                                        transform = ToTensor()),\n",
    "                        batch_size = TrainBS,\n",
    "                        shuffle = True)\n",
    "Test_Data = DataLoader(dataset = MNIST(train = False,\n",
    "                                        root = '/data/',\n",
    "                                        download = True,\n",
    "                                        transform = ToTensor()),\n",
    "                        batch_size = TestBS,\n",
    "                        shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4363374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher_Model(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(Teacher_Model, self).__init__()\n",
    "        self.fc1 = Linear(784, 784)         \n",
    "        self.fc2 = Linear(784, 1200)\n",
    "        self.fc3 = Linear(1200, num_classes)\n",
    "        self.relu = ReLU()\n",
    "        self.dropout = Dropout(p = 0.5)\n",
    "    def forward(self,x):\n",
    "        x = x.view (-1,784)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a56c2321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 784]         615,440\n",
      "           Dropout-2                  [-1, 784]               0\n",
      "              ReLU-3                  [-1, 784]               0\n",
      "            Linear-4                 [-1, 1200]         942,000\n",
      "           Dropout-5                 [-1, 1200]               0\n",
      "              ReLU-6                 [-1, 1200]               0\n",
      "            Linear-7                   [-1, 10]          12,010\n",
      "================================================================\n",
      "Total params: 1,569,450\n",
      "Trainable params: 1,569,450\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 5.99\n",
      "Estimated Total Size (MB): 6.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Model = Teacher_Model(num_classes = 10)\n",
    "summary(Model, (1,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "352b9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = Teacher_Model()\n",
    "optimizer = Adam(params = Model.parameters(),lr=1e-2)  \n",
    "loss_function = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57ca5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [0/60000 (0%)]\t Loss:0.558514\t Acc:86.718750\n",
      "train epoch:0 [128/60000 (0%)]\t Loss:0.926196\t Acc:87.500000\n",
      "train epoch:0 [256/60000 (0%)]\t Loss:0.739337\t Acc:89.062500\n",
      "train epoch:0 [384/60000 (1%)]\t Loss:0.586004\t Acc:86.718750\n",
      "train epoch:0 [512/60000 (1%)]\t Loss:0.715725\t Acc:85.156250\n",
      "train epoch:0 [640/60000 (1%)]\t Loss:0.609133\t Acc:87.500000\n",
      "train epoch:0 [768/60000 (1%)]\t Loss:0.512858\t Acc:84.375000\n",
      "train epoch:0 [896/60000 (1%)]\t Loss:0.576244\t Acc:85.937500\n",
      "train epoch:0 [1024/60000 (2%)]\t Loss:0.486361\t Acc:85.937500\n",
      "train epoch:0 [1152/60000 (2%)]\t Loss:0.375317\t Acc:91.406250\n",
      "train epoch:0 [1280/60000 (2%)]\t Loss:0.385579\t Acc:86.718750\n",
      "train epoch:0 [1408/60000 (2%)]\t Loss:0.558676\t Acc:83.593750\n",
      "train epoch:0 [1536/60000 (3%)]\t Loss:0.618531\t Acc:85.937500\n",
      "train epoch:0 [1664/60000 (3%)]\t Loss:1.471763\t Acc:80.468750\n",
      "train epoch:0 [1792/60000 (3%)]\t Loss:0.498670\t Acc:86.718750\n",
      "train epoch:0 [1920/60000 (3%)]\t Loss:0.715192\t Acc:84.375000\n",
      "train epoch:0 [2048/60000 (3%)]\t Loss:0.278383\t Acc:89.843750\n",
      "train epoch:0 [2176/60000 (4%)]\t Loss:0.723958\t Acc:85.156250\n",
      "train epoch:0 [2304/60000 (4%)]\t Loss:0.483131\t Acc:83.593750\n",
      "train epoch:0 [2432/60000 (4%)]\t Loss:0.890035\t Acc:85.937500\n",
      "train epoch:0 [2560/60000 (4%)]\t Loss:0.304156\t Acc:92.187500\n",
      "train epoch:0 [2688/60000 (4%)]\t Loss:0.656816\t Acc:89.062500\n",
      "train epoch:0 [2816/60000 (5%)]\t Loss:0.485290\t Acc:82.031250\n",
      "train epoch:0 [2944/60000 (5%)]\t Loss:0.564774\t Acc:87.500000\n",
      "train epoch:0 [3072/60000 (5%)]\t Loss:0.720391\t Acc:79.687500\n",
      "train epoch:0 [3200/60000 (5%)]\t Loss:0.568492\t Acc:84.375000\n",
      "train epoch:0 [3328/60000 (6%)]\t Loss:0.296531\t Acc:89.062500\n",
      "train epoch:0 [3456/60000 (6%)]\t Loss:0.903645\t Acc:83.593750\n",
      "train epoch:0 [3584/60000 (6%)]\t Loss:0.695590\t Acc:81.250000\n",
      "train epoch:0 [3712/60000 (6%)]\t Loss:0.520737\t Acc:86.718750\n",
      "train epoch:0 [3840/60000 (6%)]\t Loss:0.428250\t Acc:85.156250\n",
      "train epoch:0 [3968/60000 (7%)]\t Loss:0.578896\t Acc:85.156250\n",
      "train epoch:0 [4096/60000 (7%)]\t Loss:0.519618\t Acc:85.156250\n",
      "train epoch:0 [4224/60000 (7%)]\t Loss:0.486445\t Acc:87.500000\n",
      "train epoch:0 [4352/60000 (7%)]\t Loss:0.539512\t Acc:82.031250\n",
      "train epoch:0 [4480/60000 (7%)]\t Loss:0.739141\t Acc:87.500000\n",
      "train epoch:0 [4608/60000 (8%)]\t Loss:0.407425\t Acc:86.718750\n",
      "train epoch:0 [4736/60000 (8%)]\t Loss:0.409773\t Acc:87.500000\n",
      "train epoch:0 [4864/60000 (8%)]\t Loss:0.526863\t Acc:82.031250\n",
      "train epoch:0 [4992/60000 (8%)]\t Loss:0.587839\t Acc:89.062500\n",
      "train epoch:0 [5120/60000 (9%)]\t Loss:0.326207\t Acc:92.968750\n",
      "train epoch:0 [5248/60000 (9%)]\t Loss:0.528412\t Acc:84.375000\n",
      "train epoch:0 [5376/60000 (9%)]\t Loss:0.523285\t Acc:82.031250\n",
      "train epoch:0 [5504/60000 (9%)]\t Loss:0.389966\t Acc:88.281250\n",
      "train epoch:0 [5632/60000 (9%)]\t Loss:0.585182\t Acc:84.375000\n",
      "train epoch:0 [5760/60000 (10%)]\t Loss:0.431973\t Acc:87.500000\n",
      "train epoch:0 [5888/60000 (10%)]\t Loss:0.330904\t Acc:88.281250\n",
      "train epoch:0 [6016/60000 (10%)]\t Loss:0.468073\t Acc:85.937500\n",
      "train epoch:0 [6144/60000 (10%)]\t Loss:0.600702\t Acc:85.937500\n",
      "train epoch:0 [6272/60000 (10%)]\t Loss:0.352654\t Acc:90.625000\n",
      "train epoch:0 [6400/60000 (11%)]\t Loss:0.616570\t Acc:82.812500\n",
      "train epoch:0 [6528/60000 (11%)]\t Loss:0.635477\t Acc:83.593750\n",
      "train epoch:0 [6656/60000 (11%)]\t Loss:0.617186\t Acc:86.718750\n",
      "train epoch:0 [6784/60000 (11%)]\t Loss:0.515441\t Acc:80.468750\n",
      "train epoch:0 [6912/60000 (12%)]\t Loss:0.508466\t Acc:86.718750\n",
      "train epoch:0 [7040/60000 (12%)]\t Loss:0.391997\t Acc:90.625000\n",
      "train epoch:0 [7168/60000 (12%)]\t Loss:0.714562\t Acc:85.937500\n",
      "train epoch:0 [7296/60000 (12%)]\t Loss:0.739881\t Acc:82.031250\n",
      "train epoch:0 [7424/60000 (12%)]\t Loss:0.413986\t Acc:85.937500\n",
      "train epoch:0 [7552/60000 (13%)]\t Loss:0.497879\t Acc:86.718750\n",
      "train epoch:0 [7680/60000 (13%)]\t Loss:0.582650\t Acc:84.375000\n",
      "train epoch:0 [7808/60000 (13%)]\t Loss:0.497540\t Acc:87.500000\n",
      "train epoch:0 [7936/60000 (13%)]\t Loss:0.450214\t Acc:87.500000\n",
      "train epoch:0 [8064/60000 (13%)]\t Loss:0.799017\t Acc:85.937500\n",
      "train epoch:0 [8192/60000 (14%)]\t Loss:0.499458\t Acc:82.812500\n",
      "train epoch:0 [8320/60000 (14%)]\t Loss:0.406065\t Acc:85.937500\n",
      "train epoch:0 [8448/60000 (14%)]\t Loss:0.518747\t Acc:85.156250\n",
      "train epoch:0 [8576/60000 (14%)]\t Loss:0.557690\t Acc:89.062500\n",
      "train epoch:0 [8704/60000 (14%)]\t Loss:0.583495\t Acc:89.062500\n",
      "train epoch:0 [8832/60000 (15%)]\t Loss:0.542574\t Acc:84.375000\n",
      "train epoch:0 [8960/60000 (15%)]\t Loss:0.610991\t Acc:82.031250\n",
      "train epoch:0 [9088/60000 (15%)]\t Loss:0.474819\t Acc:85.156250\n",
      "train epoch:0 [9216/60000 (15%)]\t Loss:0.499067\t Acc:83.593750\n",
      "train epoch:0 [9344/60000 (16%)]\t Loss:0.723347\t Acc:85.937500\n",
      "train epoch:0 [9472/60000 (16%)]\t Loss:0.387739\t Acc:88.281250\n",
      "train epoch:0 [9600/60000 (16%)]\t Loss:1.095240\t Acc:83.593750\n",
      "train epoch:0 [9728/60000 (16%)]\t Loss:0.355952\t Acc:89.062500\n",
      "train epoch:0 [9856/60000 (16%)]\t Loss:0.496808\t Acc:81.250000\n",
      "train epoch:0 [9984/60000 (17%)]\t Loss:0.433241\t Acc:88.281250\n",
      "train epoch:0 [10112/60000 (17%)]\t Loss:1.311840\t Acc:88.281250\n",
      "train epoch:0 [10240/60000 (17%)]\t Loss:0.337917\t Acc:85.937500\n",
      "train epoch:0 [10368/60000 (17%)]\t Loss:0.511053\t Acc:82.031250\n",
      "train epoch:0 [10496/60000 (17%)]\t Loss:0.574750\t Acc:82.812500\n",
      "train epoch:0 [10624/60000 (18%)]\t Loss:0.420387\t Acc:87.500000\n",
      "train epoch:0 [10752/60000 (18%)]\t Loss:0.757210\t Acc:87.500000\n",
      "train epoch:0 [10880/60000 (18%)]\t Loss:0.654026\t Acc:85.156250\n",
      "train epoch:0 [11008/60000 (18%)]\t Loss:0.347965\t Acc:89.062500\n",
      "train epoch:0 [11136/60000 (19%)]\t Loss:0.537162\t Acc:84.375000\n",
      "train epoch:0 [11264/60000 (19%)]\t Loss:0.482990\t Acc:86.718750\n",
      "train epoch:0 [11392/60000 (19%)]\t Loss:0.482014\t Acc:85.156250\n",
      "train epoch:0 [11520/60000 (19%)]\t Loss:0.486160\t Acc:82.812500\n",
      "train epoch:0 [11648/60000 (19%)]\t Loss:0.738729\t Acc:83.593750\n",
      "train epoch:0 [11776/60000 (20%)]\t Loss:0.478854\t Acc:82.812500\n",
      "train epoch:0 [11904/60000 (20%)]\t Loss:0.653161\t Acc:82.812500\n",
      "train epoch:0 [12032/60000 (20%)]\t Loss:0.621196\t Acc:86.718750\n",
      "train epoch:0 [12160/60000 (20%)]\t Loss:1.031816\t Acc:86.718750\n",
      "train epoch:0 [12288/60000 (20%)]\t Loss:0.400785\t Acc:86.718750\n",
      "train epoch:0 [12416/60000 (21%)]\t Loss:0.415650\t Acc:91.406250\n",
      "train epoch:0 [12544/60000 (21%)]\t Loss:0.319997\t Acc:89.843750\n",
      "train epoch:0 [12672/60000 (21%)]\t Loss:0.582036\t Acc:85.156250\n",
      "train epoch:0 [12800/60000 (21%)]\t Loss:0.492557\t Acc:84.375000\n",
      "train epoch:0 [12928/60000 (22%)]\t Loss:0.519305\t Acc:86.718750\n",
      "train epoch:0 [13056/60000 (22%)]\t Loss:0.668550\t Acc:89.843750\n",
      "train epoch:0 [13184/60000 (22%)]\t Loss:0.965463\t Acc:80.468750\n",
      "train epoch:0 [13312/60000 (22%)]\t Loss:0.703111\t Acc:83.593750\n",
      "train epoch:0 [13440/60000 (22%)]\t Loss:0.468897\t Acc:85.156250\n",
      "train epoch:0 [13568/60000 (23%)]\t Loss:0.954195\t Acc:79.687500\n",
      "train epoch:0 [13696/60000 (23%)]\t Loss:0.483537\t Acc:88.281250\n",
      "train epoch:0 [13824/60000 (23%)]\t Loss:0.567219\t Acc:86.718750\n",
      "train epoch:0 [13952/60000 (23%)]\t Loss:0.683523\t Acc:83.593750\n",
      "train epoch:0 [14080/60000 (23%)]\t Loss:0.913920\t Acc:76.562500\n",
      "train epoch:0 [14208/60000 (24%)]\t Loss:0.633817\t Acc:80.468750\n",
      "train epoch:0 [14336/60000 (24%)]\t Loss:0.506252\t Acc:86.718750\n",
      "train epoch:0 [14464/60000 (24%)]\t Loss:0.429698\t Acc:87.500000\n",
      "train epoch:0 [14592/60000 (24%)]\t Loss:0.671345\t Acc:80.468750\n",
      "train epoch:0 [14720/60000 (25%)]\t Loss:0.436513\t Acc:87.500000\n",
      "train epoch:0 [14848/60000 (25%)]\t Loss:0.619680\t Acc:82.031250\n",
      "train epoch:0 [14976/60000 (25%)]\t Loss:0.831495\t Acc:75.000000\n",
      "train epoch:0 [15104/60000 (25%)]\t Loss:0.712330\t Acc:84.375000\n",
      "train epoch:0 [15232/60000 (25%)]\t Loss:0.486508\t Acc:85.937500\n",
      "train epoch:0 [15360/60000 (26%)]\t Loss:0.316956\t Acc:90.625000\n",
      "train epoch:0 [15488/60000 (26%)]\t Loss:0.403942\t Acc:87.500000\n",
      "train epoch:0 [15616/60000 (26%)]\t Loss:0.525748\t Acc:89.062500\n",
      "train epoch:0 [15744/60000 (26%)]\t Loss:0.375857\t Acc:85.937500\n",
      "train epoch:0 [15872/60000 (26%)]\t Loss:0.712092\t Acc:83.593750\n",
      "train epoch:0 [16000/60000 (27%)]\t Loss:0.626415\t Acc:85.156250\n",
      "train epoch:0 [16128/60000 (27%)]\t Loss:0.728274\t Acc:86.718750\n",
      "train epoch:0 [16256/60000 (27%)]\t Loss:0.348457\t Acc:88.281250\n",
      "train epoch:0 [16384/60000 (27%)]\t Loss:0.561821\t Acc:80.468750\n",
      "train epoch:0 [16512/60000 (28%)]\t Loss:1.149921\t Acc:82.031250\n",
      "train epoch:0 [16640/60000 (28%)]\t Loss:0.747524\t Acc:81.250000\n",
      "train epoch:0 [16768/60000 (28%)]\t Loss:0.622477\t Acc:81.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [16896/60000 (28%)]\t Loss:0.757797\t Acc:82.812500\n",
      "train epoch:0 [17024/60000 (28%)]\t Loss:0.431130\t Acc:87.500000\n",
      "train epoch:0 [17152/60000 (29%)]\t Loss:0.450655\t Acc:84.375000\n",
      "train epoch:0 [17280/60000 (29%)]\t Loss:0.533318\t Acc:84.375000\n",
      "train epoch:0 [17408/60000 (29%)]\t Loss:0.678473\t Acc:78.906250\n",
      "train epoch:0 [17536/60000 (29%)]\t Loss:0.646684\t Acc:81.250000\n",
      "train epoch:0 [17664/60000 (29%)]\t Loss:0.628997\t Acc:80.468750\n",
      "train epoch:0 [17792/60000 (30%)]\t Loss:0.366479\t Acc:89.843750\n",
      "train epoch:0 [17920/60000 (30%)]\t Loss:0.854530\t Acc:81.250000\n",
      "train epoch:0 [18048/60000 (30%)]\t Loss:0.419209\t Acc:85.937500\n",
      "train epoch:0 [18176/60000 (30%)]\t Loss:0.549184\t Acc:85.156250\n",
      "train epoch:0 [18304/60000 (30%)]\t Loss:0.560333\t Acc:84.375000\n",
      "train epoch:0 [18432/60000 (31%)]\t Loss:0.706439\t Acc:82.812500\n",
      "train epoch:0 [18560/60000 (31%)]\t Loss:0.383380\t Acc:89.843750\n",
      "train epoch:0 [18688/60000 (31%)]\t Loss:0.493503\t Acc:82.812500\n",
      "train epoch:0 [18816/60000 (31%)]\t Loss:0.506967\t Acc:78.906250\n",
      "train epoch:0 [18944/60000 (32%)]\t Loss:0.667573\t Acc:79.687500\n",
      "train epoch:0 [19072/60000 (32%)]\t Loss:0.410090\t Acc:87.500000\n",
      "train epoch:0 [19200/60000 (32%)]\t Loss:0.796112\t Acc:79.687500\n",
      "train epoch:0 [19328/60000 (32%)]\t Loss:0.545369\t Acc:82.812500\n",
      "train epoch:0 [19456/60000 (32%)]\t Loss:0.399588\t Acc:86.718750\n",
      "train epoch:0 [19584/60000 (33%)]\t Loss:0.791125\t Acc:81.250000\n",
      "train epoch:0 [19712/60000 (33%)]\t Loss:0.635932\t Acc:79.687500\n",
      "train epoch:0 [19840/60000 (33%)]\t Loss:0.725837\t Acc:79.687500\n",
      "train epoch:0 [19968/60000 (33%)]\t Loss:0.383562\t Acc:86.718750\n",
      "train epoch:0 [20096/60000 (33%)]\t Loss:0.390676\t Acc:84.375000\n",
      "train epoch:0 [20224/60000 (34%)]\t Loss:0.704394\t Acc:82.812500\n",
      "train epoch:0 [20352/60000 (34%)]\t Loss:0.657902\t Acc:81.250000\n",
      "train epoch:0 [20480/60000 (34%)]\t Loss:0.444139\t Acc:87.500000\n",
      "train epoch:0 [20608/60000 (34%)]\t Loss:0.553167\t Acc:82.031250\n",
      "train epoch:0 [20736/60000 (35%)]\t Loss:0.588457\t Acc:85.156250\n",
      "train epoch:0 [20864/60000 (35%)]\t Loss:0.385445\t Acc:88.281250\n",
      "train epoch:0 [20992/60000 (35%)]\t Loss:0.494635\t Acc:85.156250\n",
      "train epoch:0 [21120/60000 (35%)]\t Loss:0.487340\t Acc:82.812500\n",
      "train epoch:0 [21248/60000 (35%)]\t Loss:0.429121\t Acc:83.593750\n",
      "train epoch:0 [21376/60000 (36%)]\t Loss:0.494954\t Acc:84.375000\n",
      "train epoch:0 [21504/60000 (36%)]\t Loss:0.569411\t Acc:85.156250\n",
      "train epoch:0 [21632/60000 (36%)]\t Loss:0.313961\t Acc:89.062500\n",
      "train epoch:0 [21760/60000 (36%)]\t Loss:0.279996\t Acc:92.968750\n",
      "train epoch:0 [21888/60000 (36%)]\t Loss:0.611173\t Acc:83.593750\n",
      "train epoch:0 [22016/60000 (37%)]\t Loss:0.990142\t Acc:84.375000\n",
      "train epoch:0 [22144/60000 (37%)]\t Loss:0.969502\t Acc:84.375000\n",
      "train epoch:0 [22272/60000 (37%)]\t Loss:0.511326\t Acc:86.718750\n",
      "train epoch:0 [22400/60000 (37%)]\t Loss:0.670928\t Acc:87.500000\n",
      "train epoch:0 [22528/60000 (38%)]\t Loss:0.409180\t Acc:86.718750\n",
      "train epoch:0 [22656/60000 (38%)]\t Loss:0.503511\t Acc:88.281250\n",
      "train epoch:0 [22784/60000 (38%)]\t Loss:0.513192\t Acc:82.812500\n",
      "train epoch:0 [22912/60000 (38%)]\t Loss:0.969892\t Acc:86.718750\n",
      "train epoch:0 [23040/60000 (38%)]\t Loss:0.513269\t Acc:84.375000\n",
      "train epoch:0 [23168/60000 (39%)]\t Loss:0.555972\t Acc:80.468750\n",
      "train epoch:0 [23296/60000 (39%)]\t Loss:0.478868\t Acc:85.156250\n",
      "train epoch:0 [23424/60000 (39%)]\t Loss:0.487825\t Acc:81.250000\n",
      "train epoch:0 [23552/60000 (39%)]\t Loss:0.406953\t Acc:89.062500\n",
      "train epoch:0 [23680/60000 (39%)]\t Loss:0.504781\t Acc:86.718750\n",
      "train epoch:0 [23808/60000 (40%)]\t Loss:0.447009\t Acc:85.156250\n",
      "train epoch:0 [23936/60000 (40%)]\t Loss:0.947334\t Acc:87.500000\n",
      "train epoch:0 [24064/60000 (40%)]\t Loss:0.634100\t Acc:79.687500\n",
      "train epoch:0 [24192/60000 (40%)]\t Loss:0.578756\t Acc:83.593750\n",
      "train epoch:0 [24320/60000 (41%)]\t Loss:0.460045\t Acc:88.281250\n",
      "train epoch:0 [24448/60000 (41%)]\t Loss:0.627810\t Acc:83.593750\n",
      "train epoch:0 [24576/60000 (41%)]\t Loss:0.487804\t Acc:83.593750\n",
      "train epoch:0 [24704/60000 (41%)]\t Loss:0.409752\t Acc:86.718750\n",
      "train epoch:0 [24832/60000 (41%)]\t Loss:0.779406\t Acc:81.250000\n",
      "train epoch:0 [24960/60000 (42%)]\t Loss:0.478837\t Acc:85.156250\n",
      "train epoch:0 [25088/60000 (42%)]\t Loss:0.612334\t Acc:81.250000\n",
      "train epoch:0 [25216/60000 (42%)]\t Loss:0.349987\t Acc:89.062500\n",
      "train epoch:0 [25344/60000 (42%)]\t Loss:0.507344\t Acc:83.593750\n",
      "train epoch:0 [25472/60000 (42%)]\t Loss:0.497440\t Acc:83.593750\n",
      "train epoch:0 [25600/60000 (43%)]\t Loss:0.470184\t Acc:85.156250\n",
      "train epoch:0 [25728/60000 (43%)]\t Loss:0.552529\t Acc:84.375000\n",
      "train epoch:0 [25856/60000 (43%)]\t Loss:0.338451\t Acc:88.281250\n",
      "train epoch:0 [25984/60000 (43%)]\t Loss:0.537171\t Acc:85.156250\n",
      "train epoch:0 [26112/60000 (43%)]\t Loss:0.581128\t Acc:81.250000\n",
      "train epoch:0 [26240/60000 (44%)]\t Loss:0.344394\t Acc:91.406250\n",
      "train epoch:0 [26368/60000 (44%)]\t Loss:0.550338\t Acc:86.718750\n",
      "train epoch:0 [26496/60000 (44%)]\t Loss:0.496690\t Acc:85.937500\n",
      "train epoch:0 [26624/60000 (44%)]\t Loss:0.537666\t Acc:85.937500\n",
      "train epoch:0 [26752/60000 (45%)]\t Loss:0.415426\t Acc:88.281250\n",
      "train epoch:0 [26880/60000 (45%)]\t Loss:0.455744\t Acc:83.593750\n",
      "train epoch:0 [27008/60000 (45%)]\t Loss:0.734988\t Acc:80.468750\n",
      "train epoch:0 [27136/60000 (45%)]\t Loss:0.431040\t Acc:86.718750\n",
      "train epoch:0 [27264/60000 (45%)]\t Loss:0.952803\t Acc:86.718750\n",
      "train epoch:0 [27392/60000 (46%)]\t Loss:0.552100\t Acc:82.812500\n",
      "train epoch:0 [27520/60000 (46%)]\t Loss:0.494722\t Acc:85.937500\n",
      "train epoch:0 [27648/60000 (46%)]\t Loss:0.712226\t Acc:81.250000\n",
      "train epoch:0 [27776/60000 (46%)]\t Loss:0.448967\t Acc:85.156250\n",
      "train epoch:0 [27904/60000 (46%)]\t Loss:0.523690\t Acc:83.593750\n",
      "train epoch:0 [28032/60000 (47%)]\t Loss:0.476528\t Acc:87.500000\n",
      "train epoch:0 [28160/60000 (47%)]\t Loss:0.505738\t Acc:92.968750\n",
      "train epoch:0 [28288/60000 (47%)]\t Loss:0.560437\t Acc:87.500000\n",
      "train epoch:0 [28416/60000 (47%)]\t Loss:0.611119\t Acc:87.500000\n",
      "train epoch:0 [28544/60000 (48%)]\t Loss:0.421515\t Acc:85.156250\n",
      "train epoch:0 [28672/60000 (48%)]\t Loss:0.649002\t Acc:86.718750\n",
      "train epoch:0 [28800/60000 (48%)]\t Loss:0.400460\t Acc:88.281250\n",
      "train epoch:0 [28928/60000 (48%)]\t Loss:0.499089\t Acc:88.281250\n",
      "train epoch:0 [29056/60000 (48%)]\t Loss:0.622168\t Acc:85.156250\n",
      "train epoch:0 [29184/60000 (49%)]\t Loss:0.533505\t Acc:86.718750\n",
      "train epoch:0 [29312/60000 (49%)]\t Loss:0.351601\t Acc:89.062500\n",
      "train epoch:0 [29440/60000 (49%)]\t Loss:0.577305\t Acc:83.593750\n",
      "train epoch:0 [29568/60000 (49%)]\t Loss:0.687551\t Acc:81.250000\n",
      "train epoch:0 [29696/60000 (49%)]\t Loss:0.637336\t Acc:83.593750\n",
      "train epoch:0 [29824/60000 (50%)]\t Loss:0.304545\t Acc:89.843750\n",
      "train epoch:0 [29952/60000 (50%)]\t Loss:0.499036\t Acc:86.718750\n",
      "train epoch:0 [30080/60000 (50%)]\t Loss:0.391766\t Acc:85.156250\n",
      "train epoch:0 [30208/60000 (50%)]\t Loss:0.267910\t Acc:89.843750\n",
      "train epoch:0 [30336/60000 (51%)]\t Loss:0.725128\t Acc:84.375000\n",
      "train epoch:0 [30464/60000 (51%)]\t Loss:0.433698\t Acc:83.593750\n",
      "train epoch:0 [30592/60000 (51%)]\t Loss:0.679154\t Acc:82.812500\n",
      "train epoch:0 [30720/60000 (51%)]\t Loss:0.324171\t Acc:89.843750\n",
      "train epoch:0 [30848/60000 (51%)]\t Loss:0.481055\t Acc:82.812500\n",
      "train epoch:0 [30976/60000 (52%)]\t Loss:0.533444\t Acc:86.718750\n",
      "train epoch:0 [31104/60000 (52%)]\t Loss:0.426031\t Acc:88.281250\n",
      "train epoch:0 [31232/60000 (52%)]\t Loss:0.542691\t Acc:85.937500\n",
      "train epoch:0 [31360/60000 (52%)]\t Loss:0.606256\t Acc:88.281250\n",
      "train epoch:0 [31488/60000 (52%)]\t Loss:0.481606\t Acc:82.031250\n",
      "train epoch:0 [31616/60000 (53%)]\t Loss:0.569929\t Acc:83.593750\n",
      "train epoch:0 [31744/60000 (53%)]\t Loss:0.428762\t Acc:89.062500\n",
      "train epoch:0 [31872/60000 (53%)]\t Loss:0.422252\t Acc:85.937500\n",
      "train epoch:0 [32000/60000 (53%)]\t Loss:0.388323\t Acc:89.062500\n",
      "train epoch:0 [32128/60000 (54%)]\t Loss:0.499321\t Acc:84.375000\n",
      "train epoch:0 [32256/60000 (54%)]\t Loss:0.499693\t Acc:85.937500\n",
      "train epoch:0 [32384/60000 (54%)]\t Loss:0.509813\t Acc:87.500000\n",
      "train epoch:0 [32512/60000 (54%)]\t Loss:0.611244\t Acc:89.062500\n",
      "train epoch:0 [32640/60000 (54%)]\t Loss:0.905136\t Acc:82.812500\n",
      "train epoch:0 [32768/60000 (55%)]\t Loss:0.474660\t Acc:84.375000\n",
      "train epoch:0 [32896/60000 (55%)]\t Loss:0.430508\t Acc:89.062500\n",
      "train epoch:0 [33024/60000 (55%)]\t Loss:0.560221\t Acc:90.625000\n",
      "train epoch:0 [33152/60000 (55%)]\t Loss:0.631641\t Acc:83.593750\n",
      "train epoch:0 [33280/60000 (55%)]\t Loss:0.911586\t Acc:83.593750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [33408/60000 (56%)]\t Loss:0.436679\t Acc:86.718750\n",
      "train epoch:0 [33536/60000 (56%)]\t Loss:0.412039\t Acc:89.062500\n",
      "train epoch:0 [33664/60000 (56%)]\t Loss:0.385726\t Acc:86.718750\n",
      "train epoch:0 [33792/60000 (56%)]\t Loss:0.560313\t Acc:82.812500\n",
      "train epoch:0 [33920/60000 (57%)]\t Loss:0.601274\t Acc:87.500000\n",
      "train epoch:0 [34048/60000 (57%)]\t Loss:0.309827\t Acc:90.625000\n",
      "train epoch:0 [34176/60000 (57%)]\t Loss:0.497384\t Acc:87.500000\n",
      "train epoch:0 [34304/60000 (57%)]\t Loss:0.652565\t Acc:85.937500\n",
      "train epoch:0 [34432/60000 (57%)]\t Loss:0.411625\t Acc:88.281250\n",
      "train epoch:0 [34560/60000 (58%)]\t Loss:0.390185\t Acc:85.937500\n",
      "train epoch:0 [34688/60000 (58%)]\t Loss:0.524117\t Acc:85.156250\n",
      "train epoch:0 [34816/60000 (58%)]\t Loss:0.346658\t Acc:90.625000\n",
      "train epoch:0 [34944/60000 (58%)]\t Loss:0.537789\t Acc:81.250000\n",
      "train epoch:0 [35072/60000 (58%)]\t Loss:0.526284\t Acc:87.500000\n",
      "train epoch:0 [35200/60000 (59%)]\t Loss:0.437650\t Acc:89.843750\n",
      "train epoch:0 [35328/60000 (59%)]\t Loss:0.373016\t Acc:90.625000\n",
      "train epoch:0 [35456/60000 (59%)]\t Loss:0.417611\t Acc:87.500000\n",
      "train epoch:0 [35584/60000 (59%)]\t Loss:0.446980\t Acc:90.625000\n",
      "train epoch:0 [35712/60000 (59%)]\t Loss:0.471575\t Acc:90.625000\n",
      "train epoch:0 [35840/60000 (60%)]\t Loss:0.369898\t Acc:90.625000\n",
      "train epoch:0 [35968/60000 (60%)]\t Loss:0.877743\t Acc:89.062500\n",
      "train epoch:0 [36096/60000 (60%)]\t Loss:0.445272\t Acc:86.718750\n",
      "train epoch:0 [36224/60000 (60%)]\t Loss:0.695887\t Acc:85.156250\n",
      "train epoch:0 [36352/60000 (61%)]\t Loss:0.448083\t Acc:86.718750\n",
      "train epoch:0 [36480/60000 (61%)]\t Loss:0.463446\t Acc:83.593750\n",
      "train epoch:0 [36608/60000 (61%)]\t Loss:0.639246\t Acc:82.031250\n",
      "train epoch:0 [36736/60000 (61%)]\t Loss:0.912739\t Acc:86.718750\n",
      "train epoch:0 [36864/60000 (61%)]\t Loss:0.696606\t Acc:82.812500\n",
      "train epoch:0 [36992/60000 (62%)]\t Loss:0.587564\t Acc:83.593750\n",
      "train epoch:0 [37120/60000 (62%)]\t Loss:0.430084\t Acc:88.281250\n",
      "train epoch:0 [37248/60000 (62%)]\t Loss:0.445950\t Acc:86.718750\n",
      "train epoch:0 [37376/60000 (62%)]\t Loss:0.304627\t Acc:90.625000\n",
      "train epoch:0 [37504/60000 (62%)]\t Loss:0.604793\t Acc:85.156250\n",
      "train epoch:0 [37632/60000 (63%)]\t Loss:0.241364\t Acc:92.968750\n",
      "train epoch:0 [37760/60000 (63%)]\t Loss:0.493514\t Acc:83.593750\n",
      "train epoch:0 [37888/60000 (63%)]\t Loss:0.481720\t Acc:91.406250\n",
      "train epoch:0 [38016/60000 (63%)]\t Loss:0.482748\t Acc:83.593750\n",
      "train epoch:0 [38144/60000 (64%)]\t Loss:0.454161\t Acc:82.031250\n",
      "train epoch:0 [38272/60000 (64%)]\t Loss:0.473220\t Acc:86.718750\n",
      "train epoch:0 [38400/60000 (64%)]\t Loss:0.729106\t Acc:85.937500\n",
      "train epoch:0 [38528/60000 (64%)]\t Loss:0.921838\t Acc:85.937500\n",
      "train epoch:0 [38656/60000 (64%)]\t Loss:0.609546\t Acc:86.718750\n",
      "train epoch:0 [38784/60000 (65%)]\t Loss:0.734296\t Acc:83.593750\n",
      "train epoch:0 [38912/60000 (65%)]\t Loss:0.412046\t Acc:89.062500\n",
      "train epoch:0 [39040/60000 (65%)]\t Loss:0.431435\t Acc:87.500000\n",
      "train epoch:0 [39168/60000 (65%)]\t Loss:0.558168\t Acc:88.281250\n",
      "train epoch:0 [39296/60000 (65%)]\t Loss:0.439538\t Acc:89.843750\n",
      "train epoch:0 [39424/60000 (66%)]\t Loss:0.413331\t Acc:86.718750\n",
      "train epoch:0 [39552/60000 (66%)]\t Loss:0.760268\t Acc:84.375000\n",
      "train epoch:0 [39680/60000 (66%)]\t Loss:0.688056\t Acc:86.718750\n",
      "train epoch:0 [39808/60000 (66%)]\t Loss:0.682812\t Acc:85.937500\n",
      "train epoch:0 [39936/60000 (67%)]\t Loss:0.473356\t Acc:85.156250\n",
      "train epoch:0 [40064/60000 (67%)]\t Loss:0.489352\t Acc:85.937500\n",
      "train epoch:0 [40192/60000 (67%)]\t Loss:0.353183\t Acc:90.625000\n",
      "train epoch:0 [40320/60000 (67%)]\t Loss:0.608130\t Acc:82.031250\n",
      "train epoch:0 [40448/60000 (67%)]\t Loss:0.462771\t Acc:85.937500\n",
      "train epoch:0 [40576/60000 (68%)]\t Loss:0.426118\t Acc:87.500000\n",
      "train epoch:0 [40704/60000 (68%)]\t Loss:0.745476\t Acc:77.343750\n",
      "train epoch:0 [40832/60000 (68%)]\t Loss:0.839857\t Acc:78.906250\n",
      "train epoch:0 [40960/60000 (68%)]\t Loss:0.489140\t Acc:85.156250\n",
      "train epoch:0 [41088/60000 (68%)]\t Loss:0.430828\t Acc:88.281250\n",
      "train epoch:0 [41216/60000 (69%)]\t Loss:0.720371\t Acc:88.281250\n",
      "train epoch:0 [41344/60000 (69%)]\t Loss:0.482423\t Acc:87.500000\n",
      "train epoch:0 [41472/60000 (69%)]\t Loss:0.424765\t Acc:89.062500\n",
      "train epoch:0 [41600/60000 (69%)]\t Loss:0.544371\t Acc:87.500000\n",
      "train epoch:0 [41728/60000 (70%)]\t Loss:0.387828\t Acc:87.500000\n",
      "train epoch:0 [41856/60000 (70%)]\t Loss:0.230960\t Acc:92.968750\n",
      "train epoch:0 [41984/60000 (70%)]\t Loss:0.409676\t Acc:87.500000\n",
      "train epoch:0 [42112/60000 (70%)]\t Loss:0.403526\t Acc:88.281250\n",
      "train epoch:0 [42240/60000 (70%)]\t Loss:0.412127\t Acc:86.718750\n",
      "train epoch:0 [42368/60000 (71%)]\t Loss:0.540252\t Acc:85.937500\n",
      "train epoch:0 [42496/60000 (71%)]\t Loss:0.494313\t Acc:83.593750\n",
      "train epoch:0 [42624/60000 (71%)]\t Loss:0.659378\t Acc:84.375000\n",
      "train epoch:0 [42752/60000 (71%)]\t Loss:0.423243\t Acc:86.718750\n",
      "train epoch:0 [42880/60000 (71%)]\t Loss:0.693032\t Acc:80.468750\n",
      "train epoch:0 [43008/60000 (72%)]\t Loss:0.854842\t Acc:82.812500\n",
      "train epoch:0 [43136/60000 (72%)]\t Loss:0.469092\t Acc:90.625000\n",
      "train epoch:0 [43264/60000 (72%)]\t Loss:0.676704\t Acc:88.281250\n",
      "train epoch:0 [43392/60000 (72%)]\t Loss:0.887734\t Acc:86.718750\n",
      "train epoch:0 [43520/60000 (72%)]\t Loss:0.885250\t Acc:82.031250\n",
      "train epoch:0 [43648/60000 (73%)]\t Loss:0.493510\t Acc:82.812500\n",
      "train epoch:0 [43776/60000 (73%)]\t Loss:0.427702\t Acc:89.062500\n",
      "train epoch:0 [43904/60000 (73%)]\t Loss:0.299723\t Acc:92.968750\n",
      "train epoch:0 [44032/60000 (73%)]\t Loss:0.490342\t Acc:85.156250\n",
      "train epoch:0 [44160/60000 (74%)]\t Loss:0.579556\t Acc:80.468750\n",
      "train epoch:0 [44288/60000 (74%)]\t Loss:0.537244\t Acc:88.281250\n",
      "train epoch:0 [44416/60000 (74%)]\t Loss:0.599205\t Acc:85.156250\n",
      "train epoch:0 [44544/60000 (74%)]\t Loss:0.552141\t Acc:82.812500\n",
      "train epoch:0 [44672/60000 (74%)]\t Loss:0.796414\t Acc:77.343750\n",
      "train epoch:0 [44800/60000 (75%)]\t Loss:0.497203\t Acc:84.375000\n",
      "train epoch:0 [44928/60000 (75%)]\t Loss:0.263373\t Acc:92.968750\n",
      "train epoch:0 [45056/60000 (75%)]\t Loss:0.449797\t Acc:85.937500\n",
      "train epoch:0 [45184/60000 (75%)]\t Loss:0.508472\t Acc:88.281250\n",
      "train epoch:0 [45312/60000 (75%)]\t Loss:0.536032\t Acc:80.468750\n",
      "train epoch:0 [45440/60000 (76%)]\t Loss:0.499717\t Acc:85.937500\n",
      "train epoch:0 [45568/60000 (76%)]\t Loss:0.402238\t Acc:85.937500\n",
      "train epoch:0 [45696/60000 (76%)]\t Loss:0.777804\t Acc:82.031250\n",
      "train epoch:0 [45824/60000 (76%)]\t Loss:0.647223\t Acc:81.250000\n",
      "train epoch:0 [45952/60000 (77%)]\t Loss:0.412375\t Acc:84.375000\n",
      "train epoch:0 [46080/60000 (77%)]\t Loss:0.725347\t Acc:79.687500\n",
      "train epoch:0 [46208/60000 (77%)]\t Loss:0.511590\t Acc:83.593750\n",
      "train epoch:0 [46336/60000 (77%)]\t Loss:0.355370\t Acc:90.625000\n",
      "train epoch:0 [46464/60000 (77%)]\t Loss:0.718647\t Acc:82.812500\n",
      "train epoch:0 [46592/60000 (78%)]\t Loss:1.122751\t Acc:86.718750\n",
      "train epoch:0 [46720/60000 (78%)]\t Loss:0.665090\t Acc:80.468750\n",
      "train epoch:0 [46848/60000 (78%)]\t Loss:1.342386\t Acc:82.031250\n",
      "train epoch:0 [46976/60000 (78%)]\t Loss:0.518257\t Acc:83.593750\n",
      "train epoch:0 [47104/60000 (78%)]\t Loss:0.702898\t Acc:81.250000\n",
      "train epoch:0 [47232/60000 (79%)]\t Loss:0.585706\t Acc:82.812500\n",
      "train epoch:0 [47360/60000 (79%)]\t Loss:0.540065\t Acc:82.812500\n",
      "train epoch:0 [47488/60000 (79%)]\t Loss:0.407296\t Acc:87.500000\n",
      "train epoch:0 [47616/60000 (79%)]\t Loss:0.473465\t Acc:86.718750\n",
      "train epoch:0 [47744/60000 (80%)]\t Loss:0.583712\t Acc:82.812500\n",
      "train epoch:0 [47872/60000 (80%)]\t Loss:0.640547\t Acc:79.687500\n",
      "train epoch:0 [48000/60000 (80%)]\t Loss:0.596421\t Acc:85.156250\n",
      "train epoch:0 [48128/60000 (80%)]\t Loss:0.346927\t Acc:88.281250\n",
      "train epoch:0 [48256/60000 (80%)]\t Loss:0.595346\t Acc:85.937500\n",
      "train epoch:0 [48384/60000 (81%)]\t Loss:0.568985\t Acc:88.281250\n",
      "train epoch:0 [48512/60000 (81%)]\t Loss:0.930943\t Acc:87.500000\n",
      "train epoch:0 [48640/60000 (81%)]\t Loss:0.415627\t Acc:85.937500\n",
      "train epoch:0 [48768/60000 (81%)]\t Loss:0.839140\t Acc:80.468750\n",
      "train epoch:0 [48896/60000 (81%)]\t Loss:0.611052\t Acc:85.156250\n",
      "train epoch:0 [49024/60000 (82%)]\t Loss:0.480683\t Acc:87.500000\n",
      "train epoch:0 [49152/60000 (82%)]\t Loss:0.283445\t Acc:89.843750\n",
      "train epoch:0 [49280/60000 (82%)]\t Loss:0.389127\t Acc:85.937500\n",
      "train epoch:0 [49408/60000 (82%)]\t Loss:0.544572\t Acc:86.718750\n",
      "train epoch:0 [49536/60000 (83%)]\t Loss:0.425737\t Acc:87.500000\n",
      "train epoch:0 [49664/60000 (83%)]\t Loss:0.490291\t Acc:89.843750\n",
      "train epoch:0 [49792/60000 (83%)]\t Loss:0.358800\t Acc:89.843750\n",
      "train epoch:0 [49920/60000 (83%)]\t Loss:0.338798\t Acc:90.625000\n",
      "train epoch:0 [50048/60000 (83%)]\t Loss:0.664856\t Acc:85.156250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [50176/60000 (84%)]\t Loss:0.573834\t Acc:85.937500\n",
      "train epoch:0 [50304/60000 (84%)]\t Loss:0.427122\t Acc:85.937500\n",
      "train epoch:0 [50432/60000 (84%)]\t Loss:0.495878\t Acc:85.937500\n",
      "train epoch:0 [50560/60000 (84%)]\t Loss:0.464094\t Acc:84.375000\n",
      "train epoch:0 [50688/60000 (84%)]\t Loss:0.555813\t Acc:85.937500\n",
      "train epoch:0 [50816/60000 (85%)]\t Loss:0.483222\t Acc:85.156250\n",
      "train epoch:0 [50944/60000 (85%)]\t Loss:0.649617\t Acc:85.937500\n",
      "train epoch:0 [51072/60000 (85%)]\t Loss:0.584635\t Acc:82.031250\n",
      "train epoch:0 [51200/60000 (85%)]\t Loss:0.537020\t Acc:82.812500\n",
      "train epoch:0 [51328/60000 (86%)]\t Loss:0.664822\t Acc:80.468750\n",
      "train epoch:0 [51456/60000 (86%)]\t Loss:0.386562\t Acc:85.156250\n",
      "train epoch:0 [51584/60000 (86%)]\t Loss:0.535955\t Acc:85.156250\n",
      "train epoch:0 [51712/60000 (86%)]\t Loss:0.863201\t Acc:89.062500\n",
      "train epoch:0 [51840/60000 (86%)]\t Loss:0.702095\t Acc:82.812500\n",
      "train epoch:0 [51968/60000 (87%)]\t Loss:0.586888\t Acc:82.031250\n",
      "train epoch:0 [52096/60000 (87%)]\t Loss:0.248023\t Acc:92.968750\n",
      "train epoch:0 [52224/60000 (87%)]\t Loss:0.587662\t Acc:85.156250\n",
      "train epoch:0 [52352/60000 (87%)]\t Loss:0.828001\t Acc:82.812500\n",
      "train epoch:0 [52480/60000 (87%)]\t Loss:0.608624\t Acc:87.500000\n",
      "train epoch:0 [52608/60000 (88%)]\t Loss:0.413511\t Acc:85.156250\n",
      "train epoch:0 [52736/60000 (88%)]\t Loss:0.574737\t Acc:85.937500\n",
      "train epoch:0 [52864/60000 (88%)]\t Loss:0.556347\t Acc:84.375000\n",
      "train epoch:0 [52992/60000 (88%)]\t Loss:0.359017\t Acc:86.718750\n",
      "train epoch:0 [53120/60000 (88%)]\t Loss:0.685228\t Acc:89.062500\n",
      "train epoch:0 [53248/60000 (89%)]\t Loss:0.571360\t Acc:86.718750\n",
      "train epoch:0 [53376/60000 (89%)]\t Loss:0.532029\t Acc:82.812500\n",
      "train epoch:0 [53504/60000 (89%)]\t Loss:0.576103\t Acc:85.156250\n",
      "train epoch:0 [53632/60000 (89%)]\t Loss:0.458377\t Acc:88.281250\n",
      "train epoch:0 [53760/60000 (90%)]\t Loss:0.501783\t Acc:86.718750\n",
      "train epoch:0 [53888/60000 (90%)]\t Loss:0.405143\t Acc:85.156250\n",
      "train epoch:0 [54016/60000 (90%)]\t Loss:0.598599\t Acc:82.812500\n",
      "train epoch:0 [54144/60000 (90%)]\t Loss:0.583281\t Acc:87.500000\n",
      "train epoch:0 [54272/60000 (90%)]\t Loss:0.431445\t Acc:87.500000\n",
      "train epoch:0 [54400/60000 (91%)]\t Loss:0.519460\t Acc:88.281250\n",
      "train epoch:0 [54528/60000 (91%)]\t Loss:0.401380\t Acc:89.062500\n",
      "train epoch:0 [54656/60000 (91%)]\t Loss:0.540858\t Acc:82.812500\n",
      "train epoch:0 [54784/60000 (91%)]\t Loss:0.561623\t Acc:83.593750\n",
      "train epoch:0 [54912/60000 (91%)]\t Loss:0.406854\t Acc:84.375000\n",
      "train epoch:0 [55040/60000 (92%)]\t Loss:0.629811\t Acc:81.250000\n",
      "train epoch:0 [55168/60000 (92%)]\t Loss:0.786154\t Acc:81.250000\n",
      "train epoch:0 [55296/60000 (92%)]\t Loss:0.424706\t Acc:87.500000\n",
      "train epoch:0 [55424/60000 (92%)]\t Loss:0.462870\t Acc:85.937500\n",
      "train epoch:0 [55552/60000 (93%)]\t Loss:0.367513\t Acc:89.843750\n",
      "train epoch:0 [55680/60000 (93%)]\t Loss:0.525564\t Acc:88.281250\n",
      "train epoch:0 [55808/60000 (93%)]\t Loss:0.673398\t Acc:83.593750\n",
      "train epoch:0 [55936/60000 (93%)]\t Loss:0.464566\t Acc:87.500000\n",
      "train epoch:0 [56064/60000 (93%)]\t Loss:0.479932\t Acc:83.593750\n",
      "train epoch:0 [56192/60000 (94%)]\t Loss:0.428623\t Acc:85.156250\n",
      "train epoch:0 [56320/60000 (94%)]\t Loss:0.614569\t Acc:84.375000\n",
      "train epoch:0 [56448/60000 (94%)]\t Loss:0.370101\t Acc:86.718750\n",
      "train epoch:0 [56576/60000 (94%)]\t Loss:0.483893\t Acc:82.031250\n",
      "train epoch:0 [56704/60000 (94%)]\t Loss:0.432460\t Acc:88.281250\n",
      "train epoch:0 [56832/60000 (95%)]\t Loss:0.508355\t Acc:87.500000\n",
      "train epoch:0 [56960/60000 (95%)]\t Loss:0.359013\t Acc:89.843750\n",
      "train epoch:0 [57088/60000 (95%)]\t Loss:0.426287\t Acc:88.281250\n",
      "train epoch:0 [57216/60000 (95%)]\t Loss:0.349682\t Acc:86.718750\n",
      "train epoch:0 [57344/60000 (96%)]\t Loss:1.000589\t Acc:81.250000\n",
      "train epoch:0 [57472/60000 (96%)]\t Loss:0.463560\t Acc:83.593750\n",
      "train epoch:0 [57600/60000 (96%)]\t Loss:0.445973\t Acc:87.500000\n",
      "train epoch:0 [57728/60000 (96%)]\t Loss:0.763970\t Acc:81.250000\n",
      "train epoch:0 [57856/60000 (96%)]\t Loss:0.415768\t Acc:87.500000\n",
      "train epoch:0 [57984/60000 (97%)]\t Loss:0.330264\t Acc:90.625000\n",
      "train epoch:0 [58112/60000 (97%)]\t Loss:0.494417\t Acc:84.375000\n",
      "train epoch:0 [58240/60000 (97%)]\t Loss:0.619688\t Acc:83.593750\n",
      "train epoch:0 [58368/60000 (97%)]\t Loss:0.642906\t Acc:86.718750\n",
      "train epoch:0 [58496/60000 (97%)]\t Loss:1.077918\t Acc:84.375000\n",
      "train epoch:0 [58624/60000 (98%)]\t Loss:1.373686\t Acc:84.375000\n",
      "train epoch:0 [58752/60000 (98%)]\t Loss:0.374361\t Acc:87.500000\n",
      "train epoch:0 [58880/60000 (98%)]\t Loss:0.562635\t Acc:82.031250\n",
      "train epoch:0 [59008/60000 (98%)]\t Loss:0.738317\t Acc:83.593750\n",
      "train epoch:0 [59136/60000 (99%)]\t Loss:0.457611\t Acc:86.718750\n",
      "train epoch:0 [59264/60000 (99%)]\t Loss:0.500069\t Acc:90.625000\n",
      "train epoch:0 [59392/60000 (99%)]\t Loss:0.507061\t Acc:86.718750\n",
      "train epoch:0 [59520/60000 (99%)]\t Loss:0.488157\t Acc:87.500000\n",
      "train epoch:0 [59648/60000 (99%)]\t Loss:0.696850\t Acc:84.375000\n",
      "train epoch:0 [59776/60000 (100%)]\t Loss:0.280546\t Acc:92.968750\n",
      "train epoch:0 [44928/60000 (100%)]\t Loss:0.465176\t Acc:64.062500\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 \n",
    "for epoch in range(epochs):\n",
    "    Model.train()\n",
    "    for batch_idx, (data,target) in enumerate(Train_Data):\n",
    "        optimizer.zero_grad()\n",
    "        pred = Model(data)\n",
    "        loss = loss_function (pred,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tmp,pred = pred.max(1)\n",
    "        correct = 0\n",
    "        correct += pred.eq(target).sum()\n",
    "        print('train epoch:{} [{}/{} ({:.0f}%)]\\t Loss:{:.6f}\\t Acc:{:.6f}'.format(\n",
    "                epoch,batch_idx*len(data),len(Train_Data.dataset),\n",
    "                100.*batch_idx/len(Train_Data),loss.item(), 100.*correct/128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "360f834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student_Model(nn.Module):\n",
    "    def __init__(self, num_class = 10):\n",
    "        super(Student_Modle,self).__init__()\n",
    "        self.fc1 = Linear(784, 20)\n",
    "        self.fc2 = Lineat(20, num_class)\n",
    "        self.relu = ReLU()\n",
    "        self.dropout = Dropout(p = 0.5)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,784)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83c2f126",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Student_Modle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Model \u001b[38;5;241m=\u001b[39m \u001b[43mStudent_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(params \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)  \n\u001b[0;32m      3\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36mStudent_Model.__init__\u001b[1;34m(self, num_class)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(\u001b[43mStudent_Modle\u001b[49m,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m Linear(\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m Lineat(\u001b[38;5;241m20\u001b[39m, num_class)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Student_Modle' is not defined"
     ]
    }
   ],
   "source": [
    "Model = Student_Model()\n",
    "optimizer = Adam(params = Model.parameters(),lr=1e-2)  \n",
    "loss_function = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fed32f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [0/60000 (0%)]\t Loss:0.466930\t Acc:85.937500\n",
      "train epoch:0 [128/60000 (0%)]\t Loss:0.657023\t Acc:85.156250\n",
      "train epoch:0 [256/60000 (0%)]\t Loss:0.656125\t Acc:83.593750\n",
      "train epoch:0 [384/60000 (1%)]\t Loss:0.314715\t Acc:89.843750\n",
      "train epoch:0 [512/60000 (1%)]\t Loss:0.503966\t Acc:85.156250\n",
      "train epoch:0 [640/60000 (1%)]\t Loss:0.520953\t Acc:82.031250\n",
      "train epoch:0 [768/60000 (1%)]\t Loss:0.344924\t Acc:92.187500\n",
      "train epoch:0 [896/60000 (1%)]\t Loss:0.389019\t Acc:89.062500\n",
      "train epoch:0 [1024/60000 (2%)]\t Loss:0.397458\t Acc:88.281250\n",
      "train epoch:0 [1152/60000 (2%)]\t Loss:0.623684\t Acc:82.812500\n",
      "train epoch:0 [1280/60000 (2%)]\t Loss:0.391032\t Acc:91.406250\n",
      "train epoch:0 [1408/60000 (2%)]\t Loss:0.628214\t Acc:85.156250\n",
      "train epoch:0 [1536/60000 (3%)]\t Loss:0.480354\t Acc:87.500000\n",
      "train epoch:0 [1664/60000 (3%)]\t Loss:0.316966\t Acc:90.625000\n",
      "train epoch:0 [1792/60000 (3%)]\t Loss:0.531705\t Acc:88.281250\n",
      "train epoch:0 [1920/60000 (3%)]\t Loss:0.484346\t Acc:85.156250\n",
      "train epoch:0 [2048/60000 (3%)]\t Loss:0.766689\t Acc:88.281250\n",
      "train epoch:0 [2176/60000 (4%)]\t Loss:0.466897\t Acc:86.718750\n",
      "train epoch:0 [2304/60000 (4%)]\t Loss:0.550064\t Acc:86.718750\n",
      "train epoch:0 [2432/60000 (4%)]\t Loss:0.843452\t Acc:87.500000\n",
      "train epoch:0 [2560/60000 (4%)]\t Loss:0.750110\t Acc:88.281250\n",
      "train epoch:0 [2688/60000 (4%)]\t Loss:0.397021\t Acc:89.062500\n",
      "train epoch:0 [2816/60000 (5%)]\t Loss:0.503162\t Acc:85.937500\n",
      "train epoch:0 [2944/60000 (5%)]\t Loss:0.352777\t Acc:85.937500\n",
      "train epoch:0 [3072/60000 (5%)]\t Loss:0.459344\t Acc:89.062500\n",
      "train epoch:0 [3200/60000 (5%)]\t Loss:0.601148\t Acc:82.812500\n",
      "train epoch:0 [3328/60000 (6%)]\t Loss:0.449606\t Acc:82.031250\n",
      "train epoch:0 [3456/60000 (6%)]\t Loss:0.339932\t Acc:90.625000\n",
      "train epoch:0 [3584/60000 (6%)]\t Loss:0.813704\t Acc:86.718750\n",
      "train epoch:0 [3712/60000 (6%)]\t Loss:0.448439\t Acc:83.593750\n",
      "train epoch:0 [3840/60000 (6%)]\t Loss:0.503988\t Acc:85.156250\n",
      "train epoch:0 [3968/60000 (7%)]\t Loss:0.464776\t Acc:88.281250\n",
      "train epoch:0 [4096/60000 (7%)]\t Loss:0.297139\t Acc:90.625000\n",
      "train epoch:0 [4224/60000 (7%)]\t Loss:0.667187\t Acc:85.156250\n",
      "train epoch:0 [4352/60000 (7%)]\t Loss:0.498975\t Acc:85.156250\n",
      "train epoch:0 [4480/60000 (7%)]\t Loss:0.843008\t Acc:82.031250\n",
      "train epoch:0 [4608/60000 (8%)]\t Loss:0.628228\t Acc:85.937500\n",
      "train epoch:0 [4736/60000 (8%)]\t Loss:0.689559\t Acc:86.718750\n",
      "train epoch:0 [4864/60000 (8%)]\t Loss:0.420450\t Acc:85.156250\n",
      "train epoch:0 [4992/60000 (8%)]\t Loss:0.470470\t Acc:85.156250\n",
      "train epoch:0 [5120/60000 (9%)]\t Loss:0.411660\t Acc:86.718750\n",
      "train epoch:0 [5248/60000 (9%)]\t Loss:0.383203\t Acc:88.281250\n",
      "train epoch:0 [5376/60000 (9%)]\t Loss:0.360127\t Acc:88.281250\n",
      "train epoch:0 [5504/60000 (9%)]\t Loss:0.387785\t Acc:89.062500\n",
      "train epoch:0 [5632/60000 (9%)]\t Loss:0.625968\t Acc:92.187500\n",
      "train epoch:0 [5760/60000 (10%)]\t Loss:0.523670\t Acc:85.937500\n",
      "train epoch:0 [5888/60000 (10%)]\t Loss:0.611231\t Acc:92.968750\n",
      "train epoch:0 [6016/60000 (10%)]\t Loss:0.591940\t Acc:85.156250\n",
      "train epoch:0 [6144/60000 (10%)]\t Loss:0.477374\t Acc:85.156250\n",
      "train epoch:0 [6272/60000 (10%)]\t Loss:0.606868\t Acc:82.031250\n",
      "train epoch:0 [6400/60000 (11%)]\t Loss:0.328152\t Acc:89.843750\n",
      "train epoch:0 [6528/60000 (11%)]\t Loss:0.582057\t Acc:85.156250\n",
      "train epoch:0 [6656/60000 (11%)]\t Loss:0.423266\t Acc:89.062500\n",
      "train epoch:0 [6784/60000 (11%)]\t Loss:0.511314\t Acc:87.500000\n",
      "train epoch:0 [6912/60000 (12%)]\t Loss:0.524471\t Acc:85.937500\n",
      "train epoch:0 [7040/60000 (12%)]\t Loss:0.613916\t Acc:86.718750\n",
      "train epoch:0 [7168/60000 (12%)]\t Loss:0.896290\t Acc:85.156250\n",
      "train epoch:0 [7296/60000 (12%)]\t Loss:0.498137\t Acc:86.718750\n",
      "train epoch:0 [7424/60000 (12%)]\t Loss:0.761680\t Acc:85.156250\n",
      "train epoch:0 [7552/60000 (13%)]\t Loss:0.451515\t Acc:86.718750\n",
      "train epoch:0 [7680/60000 (13%)]\t Loss:0.696641\t Acc:82.031250\n",
      "train epoch:0 [7808/60000 (13%)]\t Loss:0.426367\t Acc:89.843750\n",
      "train epoch:0 [7936/60000 (13%)]\t Loss:0.371532\t Acc:87.500000\n",
      "train epoch:0 [8064/60000 (13%)]\t Loss:0.725269\t Acc:84.375000\n",
      "train epoch:0 [8192/60000 (14%)]\t Loss:0.715506\t Acc:88.281250\n",
      "train epoch:0 [8320/60000 (14%)]\t Loss:0.449367\t Acc:85.937500\n",
      "train epoch:0 [8448/60000 (14%)]\t Loss:0.352143\t Acc:89.062500\n",
      "train epoch:0 [8576/60000 (14%)]\t Loss:0.424433\t Acc:92.187500\n",
      "train epoch:0 [8704/60000 (14%)]\t Loss:0.497538\t Acc:84.375000\n",
      "train epoch:0 [8832/60000 (15%)]\t Loss:0.394244\t Acc:86.718750\n",
      "train epoch:0 [8960/60000 (15%)]\t Loss:0.437357\t Acc:87.500000\n",
      "train epoch:0 [9088/60000 (15%)]\t Loss:0.558872\t Acc:84.375000\n",
      "train epoch:0 [9216/60000 (15%)]\t Loss:0.724285\t Acc:82.812500\n",
      "train epoch:0 [9344/60000 (16%)]\t Loss:0.507760\t Acc:82.031250\n",
      "train epoch:0 [9472/60000 (16%)]\t Loss:0.511831\t Acc:85.156250\n",
      "train epoch:0 [9600/60000 (16%)]\t Loss:0.532868\t Acc:86.718750\n",
      "train epoch:0 [9728/60000 (16%)]\t Loss:1.035097\t Acc:85.156250\n",
      "train epoch:0 [9856/60000 (16%)]\t Loss:0.596097\t Acc:85.937500\n",
      "train epoch:0 [9984/60000 (17%)]\t Loss:0.445112\t Acc:93.750000\n",
      "train epoch:0 [10112/60000 (17%)]\t Loss:0.388530\t Acc:85.937500\n",
      "train epoch:0 [10240/60000 (17%)]\t Loss:0.580011\t Acc:85.937500\n",
      "train epoch:0 [10368/60000 (17%)]\t Loss:0.552194\t Acc:88.281250\n",
      "train epoch:0 [10496/60000 (17%)]\t Loss:0.246139\t Acc:91.406250\n",
      "train epoch:0 [10624/60000 (18%)]\t Loss:0.763415\t Acc:85.156250\n",
      "train epoch:0 [10752/60000 (18%)]\t Loss:0.412628\t Acc:86.718750\n",
      "train epoch:0 [10880/60000 (18%)]\t Loss:0.635868\t Acc:81.250000\n",
      "train epoch:0 [11008/60000 (18%)]\t Loss:0.501000\t Acc:82.812500\n",
      "train epoch:0 [11136/60000 (19%)]\t Loss:0.329724\t Acc:88.281250\n",
      "train epoch:0 [11264/60000 (19%)]\t Loss:0.374151\t Acc:87.500000\n",
      "train epoch:0 [11392/60000 (19%)]\t Loss:0.591391\t Acc:83.593750\n",
      "train epoch:0 [11520/60000 (19%)]\t Loss:0.566906\t Acc:81.250000\n",
      "train epoch:0 [11648/60000 (19%)]\t Loss:0.460913\t Acc:85.937500\n",
      "train epoch:0 [11776/60000 (20%)]\t Loss:0.503364\t Acc:87.500000\n",
      "train epoch:0 [11904/60000 (20%)]\t Loss:0.886157\t Acc:82.031250\n",
      "train epoch:0 [12032/60000 (20%)]\t Loss:0.510454\t Acc:82.812500\n",
      "train epoch:0 [12160/60000 (20%)]\t Loss:0.406107\t Acc:87.500000\n",
      "train epoch:0 [12288/60000 (20%)]\t Loss:0.728358\t Acc:86.718750\n",
      "train epoch:0 [12416/60000 (21%)]\t Loss:0.489223\t Acc:84.375000\n",
      "train epoch:0 [12544/60000 (21%)]\t Loss:0.644007\t Acc:80.468750\n",
      "train epoch:0 [12672/60000 (21%)]\t Loss:0.551342\t Acc:83.593750\n",
      "train epoch:0 [12800/60000 (21%)]\t Loss:0.322886\t Acc:89.062500\n",
      "train epoch:0 [12928/60000 (22%)]\t Loss:0.887150\t Acc:84.375000\n",
      "train epoch:0 [13056/60000 (22%)]\t Loss:0.457624\t Acc:86.718750\n",
      "train epoch:0 [13184/60000 (22%)]\t Loss:0.408989\t Acc:85.937500\n",
      "train epoch:0 [13312/60000 (22%)]\t Loss:0.531203\t Acc:88.281250\n",
      "train epoch:0 [13440/60000 (22%)]\t Loss:1.245644\t Acc:83.593750\n",
      "train epoch:0 [13568/60000 (23%)]\t Loss:0.544369\t Acc:82.812500\n",
      "train epoch:0 [13696/60000 (23%)]\t Loss:0.455831\t Acc:85.937500\n",
      "train epoch:0 [13824/60000 (23%)]\t Loss:0.491464\t Acc:88.281250\n",
      "train epoch:0 [13952/60000 (23%)]\t Loss:0.419982\t Acc:90.625000\n",
      "train epoch:0 [14080/60000 (23%)]\t Loss:0.564475\t Acc:88.281250\n",
      "train epoch:0 [14208/60000 (24%)]\t Loss:0.515546\t Acc:86.718750\n",
      "train epoch:0 [14336/60000 (24%)]\t Loss:0.494939\t Acc:84.375000\n",
      "train epoch:0 [14464/60000 (24%)]\t Loss:0.597688\t Acc:85.937500\n",
      "train epoch:0 [14592/60000 (24%)]\t Loss:0.233564\t Acc:91.406250\n",
      "train epoch:0 [14720/60000 (25%)]\t Loss:0.540705\t Acc:86.718750\n",
      "train epoch:0 [14848/60000 (25%)]\t Loss:0.517923\t Acc:85.937500\n",
      "train epoch:0 [14976/60000 (25%)]\t Loss:0.482211\t Acc:85.156250\n",
      "train epoch:0 [15104/60000 (25%)]\t Loss:0.420622\t Acc:89.062500\n",
      "train epoch:0 [15232/60000 (25%)]\t Loss:0.711908\t Acc:82.031250\n",
      "train epoch:0 [15360/60000 (26%)]\t Loss:0.964960\t Acc:82.812500\n",
      "train epoch:0 [15488/60000 (26%)]\t Loss:0.512719\t Acc:85.937500\n",
      "train epoch:0 [15616/60000 (26%)]\t Loss:0.497970\t Acc:88.281250\n",
      "train epoch:0 [15744/60000 (26%)]\t Loss:0.627917\t Acc:78.906250\n",
      "train epoch:0 [15872/60000 (26%)]\t Loss:0.460904\t Acc:87.500000\n",
      "train epoch:0 [16000/60000 (27%)]\t Loss:0.392264\t Acc:90.625000\n",
      "train epoch:0 [16128/60000 (27%)]\t Loss:0.343278\t Acc:90.625000\n",
      "train epoch:0 [16256/60000 (27%)]\t Loss:0.577411\t Acc:87.500000\n",
      "train epoch:0 [16384/60000 (27%)]\t Loss:0.550246\t Acc:85.156250\n",
      "train epoch:0 [16512/60000 (28%)]\t Loss:0.412506\t Acc:86.718750\n",
      "train epoch:0 [16640/60000 (28%)]\t Loss:0.396150\t Acc:89.062500\n",
      "train epoch:0 [16768/60000 (28%)]\t Loss:0.638845\t Acc:81.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [16896/60000 (28%)]\t Loss:0.442331\t Acc:88.281250\n",
      "train epoch:0 [17024/60000 (28%)]\t Loss:0.584996\t Acc:89.062500\n",
      "train epoch:0 [17152/60000 (29%)]\t Loss:0.378662\t Acc:86.718750\n",
      "train epoch:0 [17280/60000 (29%)]\t Loss:0.381775\t Acc:92.968750\n",
      "train epoch:0 [17408/60000 (29%)]\t Loss:0.817598\t Acc:83.593750\n",
      "train epoch:0 [17536/60000 (29%)]\t Loss:0.877831\t Acc:85.937500\n",
      "train epoch:0 [17664/60000 (29%)]\t Loss:0.472484\t Acc:85.156250\n",
      "train epoch:0 [17792/60000 (30%)]\t Loss:0.476494\t Acc:90.625000\n",
      "train epoch:0 [17920/60000 (30%)]\t Loss:0.961609\t Acc:85.156250\n",
      "train epoch:0 [18048/60000 (30%)]\t Loss:0.473252\t Acc:87.500000\n",
      "train epoch:0 [18176/60000 (30%)]\t Loss:1.398794\t Acc:89.843750\n",
      "train epoch:0 [18304/60000 (30%)]\t Loss:0.795953\t Acc:87.500000\n",
      "train epoch:0 [18432/60000 (31%)]\t Loss:0.408220\t Acc:86.718750\n",
      "train epoch:0 [18560/60000 (31%)]\t Loss:0.934772\t Acc:85.937500\n",
      "train epoch:0 [18688/60000 (31%)]\t Loss:0.280915\t Acc:90.625000\n",
      "train epoch:0 [18816/60000 (31%)]\t Loss:0.649788\t Acc:84.375000\n",
      "train epoch:0 [18944/60000 (32%)]\t Loss:0.269077\t Acc:92.968750\n",
      "train epoch:0 [19072/60000 (32%)]\t Loss:0.593616\t Acc:86.718750\n",
      "train epoch:0 [19200/60000 (32%)]\t Loss:0.497337\t Acc:89.062500\n",
      "train epoch:0 [19328/60000 (32%)]\t Loss:0.425814\t Acc:88.281250\n",
      "train epoch:0 [19456/60000 (32%)]\t Loss:0.362742\t Acc:90.625000\n",
      "train epoch:0 [19584/60000 (33%)]\t Loss:0.487738\t Acc:86.718750\n",
      "train epoch:0 [19712/60000 (33%)]\t Loss:0.359160\t Acc:90.625000\n",
      "train epoch:0 [19840/60000 (33%)]\t Loss:0.346244\t Acc:89.062500\n",
      "train epoch:0 [19968/60000 (33%)]\t Loss:0.624479\t Acc:89.062500\n",
      "train epoch:0 [20096/60000 (33%)]\t Loss:0.625907\t Acc:88.281250\n",
      "train epoch:0 [20224/60000 (34%)]\t Loss:0.701655\t Acc:75.000000\n",
      "train epoch:0 [20352/60000 (34%)]\t Loss:0.291242\t Acc:91.406250\n",
      "train epoch:0 [20480/60000 (34%)]\t Loss:0.838647\t Acc:85.156250\n",
      "train epoch:0 [20608/60000 (34%)]\t Loss:0.790807\t Acc:82.812500\n",
      "train epoch:0 [20736/60000 (35%)]\t Loss:0.520165\t Acc:82.031250\n",
      "train epoch:0 [20864/60000 (35%)]\t Loss:0.560070\t Acc:88.281250\n",
      "train epoch:0 [20992/60000 (35%)]\t Loss:0.555411\t Acc:85.156250\n",
      "train epoch:0 [21120/60000 (35%)]\t Loss:0.666456\t Acc:86.718750\n",
      "train epoch:0 [21248/60000 (35%)]\t Loss:0.567057\t Acc:86.718750\n",
      "train epoch:0 [21376/60000 (36%)]\t Loss:0.483496\t Acc:89.062500\n",
      "train epoch:0 [21504/60000 (36%)]\t Loss:0.501933\t Acc:83.593750\n",
      "train epoch:0 [21632/60000 (36%)]\t Loss:1.066635\t Acc:89.062500\n",
      "train epoch:0 [21760/60000 (36%)]\t Loss:0.461617\t Acc:85.156250\n",
      "train epoch:0 [21888/60000 (36%)]\t Loss:0.497688\t Acc:86.718750\n",
      "train epoch:0 [22016/60000 (37%)]\t Loss:0.688974\t Acc:82.812500\n",
      "train epoch:0 [22144/60000 (37%)]\t Loss:0.589965\t Acc:79.687500\n",
      "train epoch:0 [22272/60000 (37%)]\t Loss:0.480152\t Acc:86.718750\n",
      "train epoch:0 [22400/60000 (37%)]\t Loss:0.428142\t Acc:86.718750\n",
      "train epoch:0 [22528/60000 (38%)]\t Loss:0.562662\t Acc:82.812500\n",
      "train epoch:0 [22656/60000 (38%)]\t Loss:0.594353\t Acc:81.250000\n",
      "train epoch:0 [22784/60000 (38%)]\t Loss:0.503949\t Acc:85.937500\n",
      "train epoch:0 [22912/60000 (38%)]\t Loss:0.441613\t Acc:89.062500\n",
      "train epoch:0 [23040/60000 (38%)]\t Loss:0.771248\t Acc:75.781250\n",
      "train epoch:0 [23168/60000 (39%)]\t Loss:0.818714\t Acc:85.156250\n",
      "train epoch:0 [23296/60000 (39%)]\t Loss:0.443918\t Acc:87.500000\n",
      "train epoch:0 [23424/60000 (39%)]\t Loss:0.421020\t Acc:85.156250\n",
      "train epoch:0 [23552/60000 (39%)]\t Loss:0.468770\t Acc:85.937500\n",
      "train epoch:0 [23680/60000 (39%)]\t Loss:0.610777\t Acc:85.156250\n",
      "train epoch:0 [23808/60000 (40%)]\t Loss:1.058651\t Acc:84.375000\n",
      "train epoch:0 [23936/60000 (40%)]\t Loss:0.419327\t Acc:88.281250\n",
      "train epoch:0 [24064/60000 (40%)]\t Loss:0.387708\t Acc:89.062500\n",
      "train epoch:0 [24192/60000 (40%)]\t Loss:0.498931\t Acc:84.375000\n",
      "train epoch:0 [24320/60000 (41%)]\t Loss:0.966285\t Acc:83.593750\n",
      "train epoch:0 [24448/60000 (41%)]\t Loss:0.461422\t Acc:85.156250\n",
      "train epoch:0 [24576/60000 (41%)]\t Loss:0.553041\t Acc:84.375000\n",
      "train epoch:0 [24704/60000 (41%)]\t Loss:0.382409\t Acc:85.937500\n",
      "train epoch:0 [24832/60000 (41%)]\t Loss:0.386426\t Acc:87.500000\n",
      "train epoch:0 [24960/60000 (42%)]\t Loss:0.570410\t Acc:85.937500\n",
      "train epoch:0 [25088/60000 (42%)]\t Loss:0.445555\t Acc:89.062500\n",
      "train epoch:0 [25216/60000 (42%)]\t Loss:0.777834\t Acc:82.812500\n",
      "train epoch:0 [25344/60000 (42%)]\t Loss:0.463664\t Acc:85.156250\n",
      "train epoch:0 [25472/60000 (42%)]\t Loss:0.455969\t Acc:85.156250\n",
      "train epoch:0 [25600/60000 (43%)]\t Loss:0.638313\t Acc:84.375000\n",
      "train epoch:0 [25728/60000 (43%)]\t Loss:0.332267\t Acc:89.843750\n",
      "train epoch:0 [25856/60000 (43%)]\t Loss:0.493357\t Acc:86.718750\n",
      "train epoch:0 [25984/60000 (43%)]\t Loss:0.671505\t Acc:85.156250\n",
      "train epoch:0 [26112/60000 (43%)]\t Loss:0.487393\t Acc:89.062500\n",
      "train epoch:0 [26240/60000 (44%)]\t Loss:0.457340\t Acc:85.156250\n",
      "train epoch:0 [26368/60000 (44%)]\t Loss:0.277172\t Acc:89.843750\n",
      "train epoch:0 [26496/60000 (44%)]\t Loss:0.816274\t Acc:85.937500\n",
      "train epoch:0 [26624/60000 (44%)]\t Loss:0.362752\t Acc:86.718750\n",
      "train epoch:0 [26752/60000 (45%)]\t Loss:0.525836\t Acc:89.062500\n",
      "train epoch:0 [26880/60000 (45%)]\t Loss:0.536050\t Acc:89.062500\n",
      "train epoch:0 [27008/60000 (45%)]\t Loss:0.750443\t Acc:83.593750\n",
      "train epoch:0 [27136/60000 (45%)]\t Loss:0.524260\t Acc:87.500000\n",
      "train epoch:0 [27264/60000 (45%)]\t Loss:0.572783\t Acc:85.937500\n",
      "train epoch:0 [27392/60000 (46%)]\t Loss:0.329632\t Acc:93.750000\n",
      "train epoch:0 [27520/60000 (46%)]\t Loss:0.457376\t Acc:88.281250\n",
      "train epoch:0 [27648/60000 (46%)]\t Loss:0.333178\t Acc:89.843750\n",
      "train epoch:0 [27776/60000 (46%)]\t Loss:0.445427\t Acc:87.500000\n",
      "train epoch:0 [27904/60000 (46%)]\t Loss:0.735750\t Acc:88.281250\n",
      "train epoch:0 [28032/60000 (47%)]\t Loss:0.557582\t Acc:82.031250\n",
      "train epoch:0 [28160/60000 (47%)]\t Loss:0.358630\t Acc:89.062500\n",
      "train epoch:0 [28288/60000 (47%)]\t Loss:0.364814\t Acc:86.718750\n",
      "train epoch:0 [28416/60000 (47%)]\t Loss:0.202394\t Acc:92.187500\n",
      "train epoch:0 [28544/60000 (48%)]\t Loss:0.819189\t Acc:85.156250\n",
      "train epoch:0 [28672/60000 (48%)]\t Loss:0.447202\t Acc:89.062500\n",
      "train epoch:0 [28800/60000 (48%)]\t Loss:0.370728\t Acc:91.406250\n",
      "train epoch:0 [28928/60000 (48%)]\t Loss:0.567920\t Acc:78.906250\n",
      "train epoch:0 [29056/60000 (48%)]\t Loss:0.870855\t Acc:86.718750\n",
      "train epoch:0 [29184/60000 (49%)]\t Loss:0.402699\t Acc:87.500000\n",
      "train epoch:0 [29312/60000 (49%)]\t Loss:0.621405\t Acc:87.500000\n",
      "train epoch:0 [29440/60000 (49%)]\t Loss:0.312033\t Acc:95.312500\n",
      "train epoch:0 [29568/60000 (49%)]\t Loss:0.554106\t Acc:85.156250\n",
      "train epoch:0 [29696/60000 (49%)]\t Loss:0.854620\t Acc:83.593750\n",
      "train epoch:0 [29824/60000 (50%)]\t Loss:0.305233\t Acc:89.843750\n",
      "train epoch:0 [29952/60000 (50%)]\t Loss:0.427850\t Acc:91.406250\n",
      "train epoch:0 [30080/60000 (50%)]\t Loss:0.466411\t Acc:89.062500\n",
      "train epoch:0 [30208/60000 (50%)]\t Loss:0.503712\t Acc:86.718750\n",
      "train epoch:0 [30336/60000 (51%)]\t Loss:0.387168\t Acc:85.156250\n",
      "train epoch:0 [30464/60000 (51%)]\t Loss:0.482914\t Acc:89.843750\n",
      "train epoch:0 [30592/60000 (51%)]\t Loss:0.523892\t Acc:87.500000\n",
      "train epoch:0 [30720/60000 (51%)]\t Loss:0.298921\t Acc:89.843750\n",
      "train epoch:0 [30848/60000 (51%)]\t Loss:0.707758\t Acc:85.156250\n",
      "train epoch:0 [30976/60000 (52%)]\t Loss:0.590682\t Acc:87.500000\n",
      "train epoch:0 [31104/60000 (52%)]\t Loss:0.481015\t Acc:88.281250\n",
      "train epoch:0 [31232/60000 (52%)]\t Loss:0.509580\t Acc:85.937500\n",
      "train epoch:0 [31360/60000 (52%)]\t Loss:0.282070\t Acc:90.625000\n",
      "train epoch:0 [31488/60000 (52%)]\t Loss:0.426018\t Acc:88.281250\n",
      "train epoch:0 [31616/60000 (53%)]\t Loss:0.432886\t Acc:87.500000\n",
      "train epoch:0 [31744/60000 (53%)]\t Loss:0.368647\t Acc:91.406250\n",
      "train epoch:0 [31872/60000 (53%)]\t Loss:0.569947\t Acc:83.593750\n",
      "train epoch:0 [32000/60000 (53%)]\t Loss:0.404385\t Acc:91.406250\n",
      "train epoch:0 [32128/60000 (54%)]\t Loss:0.352646\t Acc:88.281250\n",
      "train epoch:0 [32256/60000 (54%)]\t Loss:0.416684\t Acc:86.718750\n",
      "train epoch:0 [32384/60000 (54%)]\t Loss:0.699880\t Acc:84.375000\n",
      "train epoch:0 [32512/60000 (54%)]\t Loss:0.890797\t Acc:82.031250\n",
      "train epoch:0 [32640/60000 (54%)]\t Loss:0.769937\t Acc:88.281250\n",
      "train epoch:0 [32768/60000 (55%)]\t Loss:0.504975\t Acc:88.281250\n",
      "train epoch:0 [32896/60000 (55%)]\t Loss:1.099619\t Acc:84.375000\n",
      "train epoch:0 [33024/60000 (55%)]\t Loss:0.725809\t Acc:84.375000\n",
      "train epoch:0 [33152/60000 (55%)]\t Loss:0.612966\t Acc:84.375000\n",
      "train epoch:0 [33280/60000 (55%)]\t Loss:0.400783\t Acc:86.718750\n",
      "train epoch:0 [33408/60000 (56%)]\t Loss:0.648467\t Acc:85.156250\n",
      "train epoch:0 [33536/60000 (56%)]\t Loss:0.925919\t Acc:85.937500\n",
      "train epoch:0 [33664/60000 (56%)]\t Loss:0.507918\t Acc:82.812500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [33792/60000 (56%)]\t Loss:0.557768\t Acc:82.031250\n",
      "train epoch:0 [33920/60000 (57%)]\t Loss:0.541071\t Acc:86.718750\n",
      "train epoch:0 [34048/60000 (57%)]\t Loss:0.458858\t Acc:86.718750\n",
      "train epoch:0 [34176/60000 (57%)]\t Loss:0.517760\t Acc:87.500000\n",
      "train epoch:0 [34304/60000 (57%)]\t Loss:0.832431\t Acc:85.156250\n",
      "train epoch:0 [34432/60000 (57%)]\t Loss:0.488002\t Acc:83.593750\n",
      "train epoch:0 [34560/60000 (58%)]\t Loss:0.411309\t Acc:87.500000\n",
      "train epoch:0 [34688/60000 (58%)]\t Loss:0.446107\t Acc:82.812500\n",
      "train epoch:0 [34816/60000 (58%)]\t Loss:0.288401\t Acc:91.406250\n",
      "train epoch:0 [34944/60000 (58%)]\t Loss:0.423876\t Acc:83.593750\n",
      "train epoch:0 [35072/60000 (58%)]\t Loss:0.587816\t Acc:82.031250\n",
      "train epoch:0 [35200/60000 (59%)]\t Loss:0.472713\t Acc:88.281250\n",
      "train epoch:0 [35328/60000 (59%)]\t Loss:0.948635\t Acc:80.468750\n",
      "train epoch:0 [35456/60000 (59%)]\t Loss:0.753891\t Acc:78.906250\n",
      "train epoch:0 [35584/60000 (59%)]\t Loss:0.744306\t Acc:86.718750\n",
      "train epoch:0 [35712/60000 (59%)]\t Loss:0.460462\t Acc:89.062500\n",
      "train epoch:0 [35840/60000 (60%)]\t Loss:1.027012\t Acc:78.125000\n",
      "train epoch:0 [35968/60000 (60%)]\t Loss:0.341259\t Acc:89.062500\n",
      "train epoch:0 [36096/60000 (60%)]\t Loss:0.628813\t Acc:82.812500\n",
      "train epoch:0 [36224/60000 (60%)]\t Loss:0.554940\t Acc:86.718750\n",
      "train epoch:0 [36352/60000 (61%)]\t Loss:0.445585\t Acc:85.937500\n",
      "train epoch:0 [36480/60000 (61%)]\t Loss:0.576892\t Acc:85.156250\n",
      "train epoch:0 [36608/60000 (61%)]\t Loss:0.419604\t Acc:89.062500\n",
      "train epoch:0 [36736/60000 (61%)]\t Loss:0.716635\t Acc:76.562500\n",
      "train epoch:0 [36864/60000 (61%)]\t Loss:0.491547\t Acc:84.375000\n",
      "train epoch:0 [36992/60000 (62%)]\t Loss:0.326336\t Acc:92.187500\n",
      "train epoch:0 [37120/60000 (62%)]\t Loss:0.320593\t Acc:93.750000\n",
      "train epoch:0 [37248/60000 (62%)]\t Loss:0.495412\t Acc:86.718750\n",
      "train epoch:0 [37376/60000 (62%)]\t Loss:0.363032\t Acc:90.625000\n",
      "train epoch:0 [37504/60000 (62%)]\t Loss:0.458605\t Acc:87.500000\n",
      "train epoch:0 [37632/60000 (63%)]\t Loss:0.516003\t Acc:85.156250\n",
      "train epoch:0 [37760/60000 (63%)]\t Loss:0.548531\t Acc:91.406250\n",
      "train epoch:0 [37888/60000 (63%)]\t Loss:0.940478\t Acc:85.937500\n",
      "train epoch:0 [38016/60000 (63%)]\t Loss:0.554744\t Acc:83.593750\n",
      "train epoch:0 [38144/60000 (64%)]\t Loss:0.590938\t Acc:83.593750\n",
      "train epoch:0 [38272/60000 (64%)]\t Loss:0.612870\t Acc:82.812500\n",
      "train epoch:0 [38400/60000 (64%)]\t Loss:0.359194\t Acc:86.718750\n",
      "train epoch:0 [38528/60000 (64%)]\t Loss:0.455292\t Acc:87.500000\n",
      "train epoch:0 [38656/60000 (64%)]\t Loss:0.385688\t Acc:90.625000\n",
      "train epoch:0 [38784/60000 (65%)]\t Loss:0.670659\t Acc:85.937500\n",
      "train epoch:0 [38912/60000 (65%)]\t Loss:0.649577\t Acc:79.687500\n",
      "train epoch:0 [39040/60000 (65%)]\t Loss:0.658528\t Acc:85.937500\n",
      "train epoch:0 [39168/60000 (65%)]\t Loss:0.487398\t Acc:87.500000\n",
      "train epoch:0 [39296/60000 (65%)]\t Loss:0.503382\t Acc:85.937500\n",
      "train epoch:0 [39424/60000 (66%)]\t Loss:0.457841\t Acc:86.718750\n",
      "train epoch:0 [39552/60000 (66%)]\t Loss:0.959975\t Acc:81.250000\n",
      "train epoch:0 [39680/60000 (66%)]\t Loss:0.802128\t Acc:78.906250\n",
      "train epoch:0 [39808/60000 (66%)]\t Loss:0.773264\t Acc:84.375000\n",
      "train epoch:0 [39936/60000 (67%)]\t Loss:0.538000\t Acc:82.031250\n",
      "train epoch:0 [40064/60000 (67%)]\t Loss:0.464656\t Acc:86.718750\n",
      "train epoch:0 [40192/60000 (67%)]\t Loss:0.330110\t Acc:88.281250\n",
      "train epoch:0 [40320/60000 (67%)]\t Loss:0.375219\t Acc:87.500000\n",
      "train epoch:0 [40448/60000 (67%)]\t Loss:0.545573\t Acc:83.593750\n",
      "train epoch:0 [40576/60000 (68%)]\t Loss:0.465487\t Acc:84.375000\n",
      "train epoch:0 [40704/60000 (68%)]\t Loss:0.472909\t Acc:86.718750\n",
      "train epoch:0 [40832/60000 (68%)]\t Loss:0.301843\t Acc:91.406250\n",
      "train epoch:0 [40960/60000 (68%)]\t Loss:0.343711\t Acc:88.281250\n",
      "train epoch:0 [41088/60000 (68%)]\t Loss:0.422368\t Acc:89.062500\n",
      "train epoch:0 [41216/60000 (69%)]\t Loss:0.441050\t Acc:86.718750\n",
      "train epoch:0 [41344/60000 (69%)]\t Loss:0.447300\t Acc:85.156250\n",
      "train epoch:0 [41472/60000 (69%)]\t Loss:0.295800\t Acc:90.625000\n",
      "train epoch:0 [41600/60000 (69%)]\t Loss:0.645929\t Acc:82.812500\n",
      "train epoch:0 [41728/60000 (70%)]\t Loss:0.507841\t Acc:82.031250\n",
      "train epoch:0 [41856/60000 (70%)]\t Loss:0.696092\t Acc:79.687500\n",
      "train epoch:0 [41984/60000 (70%)]\t Loss:0.672135\t Acc:80.468750\n",
      "train epoch:0 [42112/60000 (70%)]\t Loss:0.482713\t Acc:86.718750\n",
      "train epoch:0 [42240/60000 (70%)]\t Loss:0.493227\t Acc:85.156250\n",
      "train epoch:0 [42368/60000 (71%)]\t Loss:0.605058\t Acc:85.937500\n",
      "train epoch:0 [42496/60000 (71%)]\t Loss:0.513950\t Acc:84.375000\n",
      "train epoch:0 [42624/60000 (71%)]\t Loss:0.627129\t Acc:82.031250\n",
      "train epoch:0 [42752/60000 (71%)]\t Loss:0.667280\t Acc:80.468750\n",
      "train epoch:0 [42880/60000 (71%)]\t Loss:0.495061\t Acc:80.468750\n",
      "train epoch:0 [43008/60000 (72%)]\t Loss:0.602986\t Acc:83.593750\n",
      "train epoch:0 [43136/60000 (72%)]\t Loss:0.742769\t Acc:83.593750\n",
      "train epoch:0 [43264/60000 (72%)]\t Loss:0.430960\t Acc:89.062500\n",
      "train epoch:0 [43392/60000 (72%)]\t Loss:0.474889\t Acc:83.593750\n",
      "train epoch:0 [43520/60000 (72%)]\t Loss:0.470973\t Acc:83.593750\n",
      "train epoch:0 [43648/60000 (73%)]\t Loss:0.382142\t Acc:87.500000\n",
      "train epoch:0 [43776/60000 (73%)]\t Loss:1.082503\t Acc:78.906250\n",
      "train epoch:0 [43904/60000 (73%)]\t Loss:0.433528\t Acc:85.156250\n",
      "train epoch:0 [44032/60000 (73%)]\t Loss:0.762069\t Acc:87.500000\n",
      "train epoch:0 [44160/60000 (74%)]\t Loss:0.473577\t Acc:88.281250\n",
      "train epoch:0 [44288/60000 (74%)]\t Loss:0.805449\t Acc:83.593750\n",
      "train epoch:0 [44416/60000 (74%)]\t Loss:0.390153\t Acc:85.937500\n",
      "train epoch:0 [44544/60000 (74%)]\t Loss:0.418222\t Acc:87.500000\n",
      "train epoch:0 [44672/60000 (74%)]\t Loss:0.443075\t Acc:89.062500\n",
      "train epoch:0 [44800/60000 (75%)]\t Loss:0.487191\t Acc:83.593750\n",
      "train epoch:0 [44928/60000 (75%)]\t Loss:1.074214\t Acc:81.250000\n",
      "train epoch:0 [45056/60000 (75%)]\t Loss:0.452733\t Acc:84.375000\n",
      "train epoch:0 [45184/60000 (75%)]\t Loss:0.383428\t Acc:89.843750\n",
      "train epoch:0 [45312/60000 (75%)]\t Loss:0.388852\t Acc:86.718750\n",
      "train epoch:0 [45440/60000 (76%)]\t Loss:0.574615\t Acc:82.031250\n",
      "train epoch:0 [45568/60000 (76%)]\t Loss:0.449214\t Acc:89.843750\n",
      "train epoch:0 [45696/60000 (76%)]\t Loss:0.636901\t Acc:82.031250\n",
      "train epoch:0 [45824/60000 (76%)]\t Loss:0.307782\t Acc:89.062500\n",
      "train epoch:0 [45952/60000 (77%)]\t Loss:0.769062\t Acc:82.812500\n",
      "train epoch:0 [46080/60000 (77%)]\t Loss:0.320052\t Acc:92.968750\n",
      "train epoch:0 [46208/60000 (77%)]\t Loss:0.278895\t Acc:92.968750\n",
      "train epoch:0 [46336/60000 (77%)]\t Loss:0.401757\t Acc:87.500000\n",
      "train epoch:0 [46464/60000 (77%)]\t Loss:0.510687\t Acc:88.281250\n",
      "train epoch:0 [46592/60000 (78%)]\t Loss:0.468057\t Acc:85.937500\n",
      "train epoch:0 [46720/60000 (78%)]\t Loss:0.526103\t Acc:82.031250\n",
      "train epoch:0 [46848/60000 (78%)]\t Loss:0.343213\t Acc:88.281250\n",
      "train epoch:0 [46976/60000 (78%)]\t Loss:0.490919\t Acc:82.031250\n",
      "train epoch:0 [47104/60000 (78%)]\t Loss:0.444286\t Acc:90.625000\n",
      "train epoch:0 [47232/60000 (79%)]\t Loss:0.675781\t Acc:86.718750\n",
      "train epoch:0 [47360/60000 (79%)]\t Loss:0.462981\t Acc:84.375000\n",
      "train epoch:0 [47488/60000 (79%)]\t Loss:0.591856\t Acc:85.156250\n",
      "train epoch:0 [47616/60000 (79%)]\t Loss:0.397818\t Acc:87.500000\n",
      "train epoch:0 [47744/60000 (80%)]\t Loss:0.358116\t Acc:88.281250\n",
      "train epoch:0 [47872/60000 (80%)]\t Loss:0.554401\t Acc:78.125000\n",
      "train epoch:0 [48000/60000 (80%)]\t Loss:0.576711\t Acc:84.375000\n",
      "train epoch:0 [48128/60000 (80%)]\t Loss:0.815718\t Acc:77.343750\n",
      "train epoch:0 [48256/60000 (80%)]\t Loss:0.399686\t Acc:85.937500\n",
      "train epoch:0 [48384/60000 (81%)]\t Loss:0.539540\t Acc:82.031250\n",
      "train epoch:0 [48512/60000 (81%)]\t Loss:0.716946\t Acc:86.718750\n",
      "train epoch:0 [48640/60000 (81%)]\t Loss:0.394175\t Acc:88.281250\n",
      "train epoch:0 [48768/60000 (81%)]\t Loss:0.472427\t Acc:88.281250\n",
      "train epoch:0 [48896/60000 (81%)]\t Loss:0.440246\t Acc:87.500000\n",
      "train epoch:0 [49024/60000 (82%)]\t Loss:0.492109\t Acc:85.937500\n",
      "train epoch:0 [49152/60000 (82%)]\t Loss:0.455832\t Acc:89.062500\n",
      "train epoch:0 [49280/60000 (82%)]\t Loss:0.471848\t Acc:86.718750\n",
      "train epoch:0 [49408/60000 (82%)]\t Loss:0.513134\t Acc:85.937500\n",
      "train epoch:0 [49536/60000 (83%)]\t Loss:0.712802\t Acc:85.156250\n",
      "train epoch:0 [49664/60000 (83%)]\t Loss:0.717284\t Acc:89.062500\n",
      "train epoch:0 [49792/60000 (83%)]\t Loss:0.988287\t Acc:76.562500\n",
      "train epoch:0 [49920/60000 (83%)]\t Loss:0.626843\t Acc:84.375000\n",
      "train epoch:0 [50048/60000 (83%)]\t Loss:0.601388\t Acc:83.593750\n",
      "train epoch:0 [50176/60000 (84%)]\t Loss:0.771831\t Acc:84.375000\n",
      "train epoch:0 [50304/60000 (84%)]\t Loss:0.596727\t Acc:84.375000\n",
      "train epoch:0 [50432/60000 (84%)]\t Loss:0.560290\t Acc:84.375000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0 [50560/60000 (84%)]\t Loss:0.500434\t Acc:83.593750\n",
      "train epoch:0 [50688/60000 (84%)]\t Loss:0.530151\t Acc:81.250000\n",
      "train epoch:0 [50816/60000 (85%)]\t Loss:0.439535\t Acc:89.062500\n",
      "train epoch:0 [50944/60000 (85%)]\t Loss:1.388795\t Acc:85.156250\n",
      "train epoch:0 [51072/60000 (85%)]\t Loss:0.642699\t Acc:81.250000\n",
      "train epoch:0 [51200/60000 (85%)]\t Loss:0.418771\t Acc:85.937500\n",
      "train epoch:0 [51328/60000 (86%)]\t Loss:0.655332\t Acc:87.500000\n",
      "train epoch:0 [51456/60000 (86%)]\t Loss:0.278230\t Acc:92.187500\n",
      "train epoch:0 [51584/60000 (86%)]\t Loss:0.305108\t Acc:89.062500\n",
      "train epoch:0 [51712/60000 (86%)]\t Loss:0.431800\t Acc:85.937500\n",
      "train epoch:0 [51840/60000 (86%)]\t Loss:0.442849\t Acc:88.281250\n",
      "train epoch:0 [51968/60000 (87%)]\t Loss:0.535033\t Acc:83.593750\n",
      "train epoch:0 [52096/60000 (87%)]\t Loss:0.614192\t Acc:86.718750\n",
      "train epoch:0 [52224/60000 (87%)]\t Loss:0.798975\t Acc:83.593750\n",
      "train epoch:0 [52352/60000 (87%)]\t Loss:0.578034\t Acc:88.281250\n",
      "train epoch:0 [52480/60000 (87%)]\t Loss:0.546750\t Acc:84.375000\n",
      "train epoch:0 [52608/60000 (88%)]\t Loss:0.416855\t Acc:92.968750\n",
      "train epoch:0 [52736/60000 (88%)]\t Loss:0.528537\t Acc:85.937500\n",
      "train epoch:0 [52864/60000 (88%)]\t Loss:0.384648\t Acc:86.718750\n",
      "train epoch:0 [52992/60000 (88%)]\t Loss:0.598100\t Acc:82.812500\n",
      "train epoch:0 [53120/60000 (88%)]\t Loss:0.467876\t Acc:86.718750\n",
      "train epoch:0 [53248/60000 (89%)]\t Loss:0.460074\t Acc:85.937500\n",
      "train epoch:0 [53376/60000 (89%)]\t Loss:0.609938\t Acc:85.937500\n",
      "train epoch:0 [53504/60000 (89%)]\t Loss:0.907151\t Acc:86.718750\n",
      "train epoch:0 [53632/60000 (89%)]\t Loss:0.533348\t Acc:85.937500\n",
      "train epoch:0 [53760/60000 (90%)]\t Loss:0.596549\t Acc:86.718750\n",
      "train epoch:0 [53888/60000 (90%)]\t Loss:0.550696\t Acc:85.937500\n",
      "train epoch:0 [54016/60000 (90%)]\t Loss:0.561486\t Acc:85.156250\n",
      "train epoch:0 [54144/60000 (90%)]\t Loss:0.522093\t Acc:87.500000\n",
      "train epoch:0 [54272/60000 (90%)]\t Loss:0.338736\t Acc:85.937500\n",
      "train epoch:0 [54400/60000 (91%)]\t Loss:0.538960\t Acc:86.718750\n",
      "train epoch:0 [54528/60000 (91%)]\t Loss:0.426326\t Acc:83.593750\n",
      "train epoch:0 [54656/60000 (91%)]\t Loss:0.286553\t Acc:92.187500\n",
      "train epoch:0 [54784/60000 (91%)]\t Loss:0.396278\t Acc:85.156250\n",
      "train epoch:0 [54912/60000 (91%)]\t Loss:0.473283\t Acc:89.062500\n",
      "train epoch:0 [55040/60000 (92%)]\t Loss:0.478591\t Acc:89.843750\n",
      "train epoch:0 [55168/60000 (92%)]\t Loss:0.548694\t Acc:85.937500\n",
      "train epoch:0 [55296/60000 (92%)]\t Loss:0.433771\t Acc:89.062500\n",
      "train epoch:0 [55424/60000 (92%)]\t Loss:0.327804\t Acc:89.062500\n",
      "train epoch:0 [55552/60000 (93%)]\t Loss:0.747617\t Acc:86.718750\n",
      "train epoch:0 [55680/60000 (93%)]\t Loss:0.666422\t Acc:84.375000\n",
      "train epoch:0 [55808/60000 (93%)]\t Loss:0.696439\t Acc:83.593750\n",
      "train epoch:0 [55936/60000 (93%)]\t Loss:0.560444\t Acc:85.937500\n",
      "train epoch:0 [56064/60000 (93%)]\t Loss:0.257578\t Acc:90.625000\n",
      "train epoch:0 [56192/60000 (94%)]\t Loss:0.437193\t Acc:84.375000\n",
      "train epoch:0 [56320/60000 (94%)]\t Loss:0.431405\t Acc:85.156250\n",
      "train epoch:0 [56448/60000 (94%)]\t Loss:0.404031\t Acc:90.625000\n",
      "train epoch:0 [56576/60000 (94%)]\t Loss:0.429935\t Acc:85.937500\n",
      "train epoch:0 [56704/60000 (94%)]\t Loss:0.668078\t Acc:78.125000\n",
      "train epoch:0 [56832/60000 (95%)]\t Loss:0.508612\t Acc:83.593750\n",
      "train epoch:0 [56960/60000 (95%)]\t Loss:0.408491\t Acc:85.937500\n",
      "train epoch:0 [57088/60000 (95%)]\t Loss:1.851003\t Acc:85.156250\n",
      "train epoch:0 [57216/60000 (95%)]\t Loss:0.590280\t Acc:85.937500\n",
      "train epoch:0 [57344/60000 (96%)]\t Loss:0.446327\t Acc:85.156250\n",
      "train epoch:0 [57472/60000 (96%)]\t Loss:0.384774\t Acc:85.156250\n",
      "train epoch:0 [57600/60000 (96%)]\t Loss:0.361025\t Acc:87.500000\n",
      "train epoch:0 [57728/60000 (96%)]\t Loss:0.324337\t Acc:91.406250\n",
      "train epoch:0 [57856/60000 (96%)]\t Loss:0.537234\t Acc:82.812500\n",
      "train epoch:0 [57984/60000 (97%)]\t Loss:0.583274\t Acc:87.500000\n",
      "train epoch:0 [58112/60000 (97%)]\t Loss:0.452305\t Acc:89.062500\n",
      "train epoch:0 [58240/60000 (97%)]\t Loss:0.385075\t Acc:92.968750\n",
      "train epoch:0 [58368/60000 (97%)]\t Loss:0.599530\t Acc:84.375000\n",
      "train epoch:0 [58496/60000 (97%)]\t Loss:0.670013\t Acc:84.375000\n",
      "train epoch:0 [58624/60000 (98%)]\t Loss:0.473015\t Acc:90.625000\n",
      "train epoch:0 [58752/60000 (98%)]\t Loss:0.391521\t Acc:89.843750\n",
      "train epoch:0 [58880/60000 (98%)]\t Loss:0.443919\t Acc:86.718750\n",
      "train epoch:0 [59008/60000 (98%)]\t Loss:0.664610\t Acc:81.250000\n",
      "train epoch:0 [59136/60000 (99%)]\t Loss:0.421233\t Acc:85.937500\n",
      "train epoch:0 [59264/60000 (99%)]\t Loss:0.474235\t Acc:86.718750\n",
      "train epoch:0 [59392/60000 (99%)]\t Loss:0.441923\t Acc:88.281250\n",
      "train epoch:0 [59520/60000 (99%)]\t Loss:0.529135\t Acc:82.812500\n",
      "train epoch:0 [59648/60000 (99%)]\t Loss:0.816197\t Acc:84.375000\n",
      "train epoch:0 [59776/60000 (100%)]\t Loss:0.691212\t Acc:89.843750\n",
      "train epoch:0 [44928/60000 (100%)]\t Loss:0.402189\t Acc:64.062500\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    Model.train()\n",
    "    for batch_idx, (data,target) in enumerate(Train_Data):\n",
    "        optimizer.zero_grad()\n",
    "        pred = Model(data)\n",
    "        loss = loss_function (pred,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tmp,pred = pred.max(1)\n",
    "        correct = 0\n",
    "        correct += pred.eq(target).sum()\n",
    "        print('train epoch:{} [{}/{} ({:.0f}%)]\\t Loss:{:.6f}\\t Acc:{:.6f}'.format(\n",
    "                epoch,batch_idx*len(data),len(Train_Data.dataset),\n",
    "                100.*batch_idx/len(Train_Data),loss.item(), 100.*correct/128))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
